{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training dataset preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['sample_submission.csv',\n 'test_identity.csv',\n 'test_transaction.csv',\n 'train_identity.csv',\n 'train_transaction.csv']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"C:/Users/verdi/Documents/Datasets/IEEE-CIS_Fraud_Detection\"\n",
    "export_path = \"C:/Users/verdi/Documents/Datasets/IEEE-CIS_Fraud_Detection/processed\"\n",
    "os.listdir(root + \"/ieee-fraud-detection\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load both training and identity tables\n",
    "train_trans_df = pd.read_csv(root + \"/ieee-fraud-detection/train_transaction.csv\", index_col='TransactionID')\n",
    "train_id_df = pd.read_csv(root + \"/ieee-fraud-detection/train_identity.csv\", index_col='TransactionID')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Merge both tables into a single table\n",
    "df_train = train_trans_df.merge(train_id_df, how='left', left_index=True, right_index=True)\n",
    "df_train.to_csv(root + \"/processed/merged/train_set.csv\")\n",
    "del train_trans_df\n",
    "del train_id_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 433 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": "               isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\nTransactionID                                                                   \n2987000              0          86400            68.5         W  13926    NaN   \n2987001              0          86401            29.0         W   2755  404.0   \n2987002              0          86469            59.0         W   4663  490.0   \n2987003              0          86499            50.0         W  18132  567.0   \n2987004              0          86506            50.0         H   4497  514.0   \n\n               card3       card4  card5   card6  ...                id_31  \\\nTransactionID                                    ...                        \n2987000        150.0    discover  142.0  credit  ...                  NaN   \n2987001        150.0  mastercard  102.0  credit  ...                  NaN   \n2987002        150.0        visa  166.0   debit  ...                  NaN   \n2987003        150.0  mastercard  117.0   debit  ...                  NaN   \n2987004        150.0  mastercard  102.0  credit  ...  samsung browser 6.2   \n\n               id_32      id_33           id_34 id_35 id_36  id_37  id_38  \\\nTransactionID                                                               \n2987000          NaN        NaN             NaN   NaN   NaN    NaN    NaN   \n2987001          NaN        NaN             NaN   NaN   NaN    NaN    NaN   \n2987002          NaN        NaN             NaN   NaN   NaN    NaN    NaN   \n2987003          NaN        NaN             NaN   NaN   NaN    NaN    NaN   \n2987004         32.0  2220x1080  match_status:2     T     F      T      T   \n\n               DeviceType                     DeviceInfo  \nTransactionID                                             \n2987000               NaN                            NaN  \n2987001               NaN                            NaN  \n2987002               NaN                            NaN  \n2987003               NaN                            NaN  \n2987004            mobile  SAMSUNG SM-G892A Build/NRD90M  \n\n[5 rows x 433 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>card6</th>\n      <th>...</th>\n      <th>id_31</th>\n      <th>id_32</th>\n      <th>id_33</th>\n      <th>id_34</th>\n      <th>id_35</th>\n      <th>id_36</th>\n      <th>id_37</th>\n      <th>id_38</th>\n      <th>DeviceType</th>\n      <th>DeviceInfo</th>\n    </tr>\n    <tr>\n      <th>TransactionID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2987000</th>\n      <td>0</td>\n      <td>86400</td>\n      <td>68.5</td>\n      <td>W</td>\n      <td>13926</td>\n      <td>NaN</td>\n      <td>150.0</td>\n      <td>discover</td>\n      <td>142.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2987001</th>\n      <td>0</td>\n      <td>86401</td>\n      <td>29.0</td>\n      <td>W</td>\n      <td>2755</td>\n      <td>404.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>102.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2987002</th>\n      <td>0</td>\n      <td>86469</td>\n      <td>59.0</td>\n      <td>W</td>\n      <td>4663</td>\n      <td>490.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>166.0</td>\n      <td>debit</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2987003</th>\n      <td>0</td>\n      <td>86499</td>\n      <td>50.0</td>\n      <td>W</td>\n      <td>18132</td>\n      <td>567.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>117.0</td>\n      <td>debit</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2987004</th>\n      <td>0</td>\n      <td>86506</td>\n      <td>50.0</td>\n      <td>H</td>\n      <td>4497</td>\n      <td>514.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>102.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>samsung browser 6.2</td>\n      <td>32.0</td>\n      <td>2220x1080</td>\n      <td>match_status:2</td>\n      <td>T</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>mobile</td>\n      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 433 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset has {} columns\".format(len(df_train.columns)))\n",
    "df_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Columns we want to keep without any preprocessing\n",
    "cols_to_ignore = ['TransactionID']\n",
    "# Anomaly column\n",
    "ad_label = 'isFraud'\n",
    "# Column used for submission\n",
    "submission_col = 'TransactionID'\n",
    "\n",
    "# Columns to drop because we do not want to use them during training\n",
    "#   - TransactionDT: We do not want to discriminate on time\n",
    "#   - card4: While certain vendors might be subject to more fraud we do not want the model to discriminate based on the card which could lead to false negatives if a frauder decides to change card\n",
    "cols_to_drop = ['TransactionDT', 'card4']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 414 columns with missing values\n",
      "Found 322 columns with missing value ratio equal or greater than 59054.00\n",
      "Remaining 92 columns have to be treated\n"
     ]
    },
    {
     "data": {
      "text/plain": "id_24    585793\nid_25    585408\nid_07    585385\nid_08    585385\nid_21    585381\n          ...  \nV285         12\nV284         12\nV280         12\nV279         12\nV312         12\nLength: 414, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "cols_missing_vals = df_train.columns[df_train.isna().sum() >= 1]\n",
    "print(\"There are {} columns with missing values\".format(len(cols_missing_vals)))\n",
    "\n",
    "# Threshold beyond which we drop features\n",
    "thresh = .10 * len(df_train)\n",
    "nan_to_drop = df_train.columns[df_train.isna().sum() >= thresh]\n",
    "cols_to_drop.extend(nan_to_drop.tolist())\n",
    "print(\"Found {} columns with missing value ratio equal or greater than {:2.2f}\".format(len(nan_to_drop), thresh))\n",
    "\n",
    "# Keep the remaining columns with missing values aside\n",
    "remaining_nan_cols = list(set(cols_missing_vals) - set(nan_to_drop))\n",
    "print(\"Remaining {} columns have to be treated\".format(len(remaining_nan_cols)))\n",
    "\n",
    "# Display all the columns with missing values\n",
    "df_train[cols_missing_vals].isna().sum().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V119', 'V314', 'V133', 'V290', 'V100', 'V137', 'D1', 'V280', 'V117',\n",
      "       'card5', 'V134', 'V126', 'V298', 'V289', 'V292', 'V286', 'V295', 'V287',\n",
      "       'V303', 'V115', 'card2', 'V125', 'V306', 'V288', 'V131', 'V113', 'V136',\n",
      "       'V297', 'V307', 'V299', 'V321', 'V103', 'V106', 'V98', 'V110', 'V112',\n",
      "       'V135', 'V316', 'V281', 'V308', 'V296', 'V127', 'V118', 'V99', 'V123',\n",
      "       'V108', 'V116', 'V107', 'V284', 'V293', 'V312', 'V102', 'V121', 'V309',\n",
      "       'V300', 'V320', 'V302', 'V313', 'V111', 'V129', 'V104', 'V132', 'V315',\n",
      "       'V124', 'V130', 'V318', 'V291', 'V105', 'V96', 'V122', 'V283', 'V304',\n",
      "       'V310', 'V305', 'V285', 'V282', 'V319', 'V109', 'V279', 'V97', 'V101',\n",
      "       'V311', 'V294', 'V128', 'V317', 'V120', 'card3', 'V95', 'V301', 'V114'],\n",
      "      dtype='object')\n",
      "Index(['card6', 'card4'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                V119           V314           V133           V290  \\\ncount  590226.000000  589271.000000  590226.000000  590528.000000   \nmean        1.000729      43.319174     204.889160       1.103011   \nstd         0.036392     173.619028    3796.316755       0.768897   \nmin         0.000000       0.000000       0.000000       1.000000   \n25%         1.000000       0.000000       0.000000       1.000000   \n50%         1.000000       0.000000       0.000000       1.000000   \n75%         1.000000       0.000000       0.000000       1.000000   \nmax         3.000000    7519.870117  133915.000000      67.000000   \n\n                V100           V137             D1           V280  \\\ncount  590226.000000  590226.000000  589271.000000  590528.000000   \nmean        0.273504      26.365090      94.347568       1.967082   \nstd         0.947176     348.332714     157.660387      27.851780   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       3.000000       0.000000   \n75%         0.000000       0.000000     122.000000       1.000000   \nmax        28.000000   90750.000000     640.000000     975.000000   \n\n                V117          card5  ...           V101           V311  \\\ncount  590226.000000  586281.000000  ...  590226.000000  590528.000000   \nmean        1.000391     199.278897  ...       0.889249       4.202175   \nstd         0.035238      41.244453  ...      20.582571     102.374938   \nmin         0.000000     100.000000  ...       0.000000       0.000000   \n25%         1.000000     166.000000  ...       0.000000       0.000000   \n50%         1.000000     226.000000  ...       0.000000       0.000000   \n75%         1.000000     226.000000  ...       0.000000       0.000000   \nmax         3.000000     237.000000  ...     869.000000   55125.000000   \n\n                V294           V128           V317           V120  \\\ncount  590528.000000  590226.000000  590528.000000  590226.000000   \nmean        2.313863     204.094038     247.606741       1.000874   \nstd        39.526468    3010.258774    3980.042828       0.041684   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       1.000000   \n50%         0.000000       0.000000       0.000000       1.000000   \n75%         0.000000       0.000000       0.000000       1.000000   \nmax      1286.000000  160000.000000  134021.000000       3.000000   \n\n               card3            V95           V301           V114  \ncount  588975.000000  590226.000000  589271.000000  590226.000000  \nmean      153.194925       1.038019       0.052002       1.009298  \nstd        11.336444      21.034304       0.318310       0.110179  \nmin       100.000000       0.000000       0.000000       0.000000  \n25%       150.000000       0.000000       0.000000       1.000000  \n50%       150.000000       0.000000       0.000000       1.000000  \n75%       150.000000       0.000000       0.000000       1.000000  \nmax       231.000000     880.000000      13.000000       6.000000  \n\n[8 rows x 90 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V119</th>\n      <th>V314</th>\n      <th>V133</th>\n      <th>V290</th>\n      <th>V100</th>\n      <th>V137</th>\n      <th>D1</th>\n      <th>V280</th>\n      <th>V117</th>\n      <th>card5</th>\n      <th>...</th>\n      <th>V101</th>\n      <th>V311</th>\n      <th>V294</th>\n      <th>V128</th>\n      <th>V317</th>\n      <th>V120</th>\n      <th>card3</th>\n      <th>V95</th>\n      <th>V301</th>\n      <th>V114</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>590226.000000</td>\n      <td>589271.000000</td>\n      <td>590226.000000</td>\n      <td>590528.000000</td>\n      <td>590226.000000</td>\n      <td>590226.000000</td>\n      <td>589271.000000</td>\n      <td>590528.000000</td>\n      <td>590226.000000</td>\n      <td>586281.000000</td>\n      <td>...</td>\n      <td>590226.000000</td>\n      <td>590528.000000</td>\n      <td>590528.000000</td>\n      <td>590226.000000</td>\n      <td>590528.000000</td>\n      <td>590226.000000</td>\n      <td>588975.000000</td>\n      <td>590226.000000</td>\n      <td>589271.000000</td>\n      <td>590226.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.000729</td>\n      <td>43.319174</td>\n      <td>204.889160</td>\n      <td>1.103011</td>\n      <td>0.273504</td>\n      <td>26.365090</td>\n      <td>94.347568</td>\n      <td>1.967082</td>\n      <td>1.000391</td>\n      <td>199.278897</td>\n      <td>...</td>\n      <td>0.889249</td>\n      <td>4.202175</td>\n      <td>2.313863</td>\n      <td>204.094038</td>\n      <td>247.606741</td>\n      <td>1.000874</td>\n      <td>153.194925</td>\n      <td>1.038019</td>\n      <td>0.052002</td>\n      <td>1.009298</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.036392</td>\n      <td>173.619028</td>\n      <td>3796.316755</td>\n      <td>0.768897</td>\n      <td>0.947176</td>\n      <td>348.332714</td>\n      <td>157.660387</td>\n      <td>27.851780</td>\n      <td>0.035238</td>\n      <td>41.244453</td>\n      <td>...</td>\n      <td>20.582571</td>\n      <td>102.374938</td>\n      <td>39.526468</td>\n      <td>3010.258774</td>\n      <td>3980.042828</td>\n      <td>0.041684</td>\n      <td>11.336444</td>\n      <td>21.034304</td>\n      <td>0.318310</td>\n      <td>0.110179</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>166.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>150.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>226.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>150.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>122.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>226.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>150.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.000000</td>\n      <td>7519.870117</td>\n      <td>133915.000000</td>\n      <td>67.000000</td>\n      <td>28.000000</td>\n      <td>90750.000000</td>\n      <td>640.000000</td>\n      <td>975.000000</td>\n      <td>3.000000</td>\n      <td>237.000000</td>\n      <td>...</td>\n      <td>869.000000</td>\n      <td>55125.000000</td>\n      <td>1286.000000</td>\n      <td>160000.000000</td>\n      <td>134021.000000</td>\n      <td>3.000000</td>\n      <td>231.000000</td>\n      <td>880.000000</td>\n      <td>13.000000</td>\n      <td>6.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 90 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the categorical from the numerical values\n",
    "num_cols_nan_vals = df_train[remaining_nan_cols].select_dtypes(exclude=['object', 'category']).columns\n",
    "cat_cols_nan_vals = df_train[remaining_nan_cols].select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Add the only categorical column with missing values to the drop list\n",
    "cols_to_drop.extend(cat_cols_nan_vals)\n",
    "print(num_cols_nan_vals)\n",
    "print(cat_cols_nan_vals)\n",
    "df_train[num_cols_nan_vals].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Use SKLearn's SimpleImputer to replace NaN values\n",
    "# Using the mean is ideal for columns with low variance\n",
    "# However for columns with high variance, another statistic should be used\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_train[num_cols_nan_vals] = imputer.fit_transform(df_train[num_cols_nan_vals].values.astype(np.float64))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 columns to be dropped\n"
     ]
    }
   ],
   "source": [
    "# Validate the previous step\n",
    "assert df_train[num_cols_nan_vals].isna().any().sum() == 0\n",
    "print(\"{} columns to be dropped\".format(len(cols_to_drop)))\n",
    "# Trick to remove duplicates\n",
    "cols_to_drop = list(set(cols_to_drop))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "               isFraud  TransactionAmt ProductCD  card1       card2  card3  \\\nTransactionID                                                                \n2987000              0            68.5         W  13926  362.555488  150.0   \n2987001              0            29.0         W   2755  404.000000  150.0   \n2987002              0            59.0         W   4663  490.000000  150.0   \n2987003              0            50.0         W  18132  567.000000  150.0   \n2987004              0            50.0         H   4497  514.000000  150.0   \n\n               card5   C1   C2   C3  ...   V312  V313  V314  V315  V316  \\\nTransactionID                        ...                                  \n2987000        142.0  1.0  1.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   \n2987001        102.0  1.0  1.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   \n2987002        166.0  1.0  1.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   \n2987003        117.0  2.0  5.0  0.0  ...  135.0   0.0   0.0   0.0  50.0   \n2987004        102.0  1.0  1.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   \n\n                 V317   V318  V319  V320  V321  \nTransactionID                                   \n2987000         117.0    0.0   0.0   0.0   0.0  \n2987001           0.0    0.0   0.0   0.0   0.0  \n2987002           0.0    0.0   0.0   0.0   0.0  \n2987003        1404.0  790.0   0.0   0.0   0.0  \n2987004           0.0    0.0   0.0   0.0   0.0  \n\n[5 rows x 108 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>...</th>\n      <th>V312</th>\n      <th>V313</th>\n      <th>V314</th>\n      <th>V315</th>\n      <th>V316</th>\n      <th>V317</th>\n      <th>V318</th>\n      <th>V319</th>\n      <th>V320</th>\n      <th>V321</th>\n    </tr>\n    <tr>\n      <th>TransactionID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2987000</th>\n      <td>0</td>\n      <td>68.5</td>\n      <td>W</td>\n      <td>13926</td>\n      <td>362.555488</td>\n      <td>150.0</td>\n      <td>142.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>117.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2987001</th>\n      <td>0</td>\n      <td>29.0</td>\n      <td>W</td>\n      <td>2755</td>\n      <td>404.000000</td>\n      <td>150.0</td>\n      <td>102.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2987002</th>\n      <td>0</td>\n      <td>59.0</td>\n      <td>W</td>\n      <td>4663</td>\n      <td>490.000000</td>\n      <td>150.0</td>\n      <td>166.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2987003</th>\n      <td>0</td>\n      <td>50.0</td>\n      <td>W</td>\n      <td>18132</td>\n      <td>567.000000</td>\n      <td>150.0</td>\n      <td>117.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>135.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>50.0</td>\n      <td>1404.0</td>\n      <td>790.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2987004</th>\n      <td>0</td>\n      <td>50.0</td>\n      <td>H</td>\n      <td>4497</td>\n      <td>514.000000</td>\n      <td>150.0</td>\n      <td>102.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 108 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the unwanted columns\n",
    "df_train = df_train.drop(columns=cols_to_drop)\n",
    "df_train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unique values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 columns with unique values\n"
     ]
    }
   ],
   "source": [
    "# Check for columns with unique values\n",
    "uniq_cols = df_train.columns[df_train.nunique() == 1]\n",
    "print(\"Dropping {} columns with unique values\".format(len(uniq_cols)))\n",
    "if len(uniq_cols) > 0:\n",
    "    df_train = df_train.drop(columns=uniq_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling 107 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": "             isFraud  TransactionAmt          card1          card2  \\\ncount  590540.000000   590540.000000  590540.000000  590540.000000   \nmean        0.034990        0.004220       0.511539       0.525111   \nstd         0.183755        0.007489       0.281741       0.313190   \nmin         0.000000        0.000000       0.000000       0.000000   \n25%         0.000000        0.001349       0.288515       0.230000   \n50%         0.000000        0.002145       0.498850       0.522000   \n75%         0.000000        0.003906       0.757875       0.824000   \nmax         1.000000        1.000000       1.000000       1.000000   \n\n               card3          card5             C1             C2  \\\ncount  590540.000000  590540.000000  590540.000000  590540.000000   \nmean        0.406068       0.724663       0.003008       0.002683   \nstd         0.086423       0.299967       0.028510       0.027178   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.381679       0.481752       0.000213       0.000176   \n50%         0.381679       0.919708       0.000213       0.000176   \n75%         0.381679       0.919708       0.000640       0.000527   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n                  C3             C4  ...           V312           V313  \\\ncount  590540.000000  590540.000000  ...  590540.000000  590540.000000   \nmean        0.000217       0.001816  ...       0.000711       0.004432   \nstd         0.005790       0.030559  ...       0.003122       0.019886   \nmin         0.000000       0.000000  ...       0.000000       0.000000   \n25%         0.000000       0.000000  ...       0.000000       0.000000   \n50%         0.000000       0.000000  ...       0.000000       0.000000   \n75%         0.000000       0.000000  ...       0.000000       0.000000   \nmax         1.000000       1.000000  ...       1.000000       1.000000   \n\n                V314           V315           V316           V317  \\\ncount  590540.000000  590540.000000  590540.000000  590540.000000   \nmean        0.005761       0.005565       0.001172       0.001848   \nstd         0.023063       0.024230       0.024217       0.029697   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n                V318           V319           V320           V321  \ncount  590540.000000  590540.000000  590540.000000  590540.000000  \nmean        0.001647       0.000177       0.000404       0.000272  \nstd         0.028365       0.003193       0.004550       0.003671  \nmin         0.000000       0.000000       0.000000       0.000000  \n25%         0.000000       0.000000       0.000000       0.000000  \n50%         0.000000       0.000000       0.000000       0.000000  \n75%         0.000000       0.000000       0.000000       0.000000  \nmax         1.000000       1.000000       1.000000       1.000000  \n\n[8 rows x 107 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>...</th>\n      <th>V312</th>\n      <th>V313</th>\n      <th>V314</th>\n      <th>V315</th>\n      <th>V316</th>\n      <th>V317</th>\n      <th>V318</th>\n      <th>V319</th>\n      <th>V320</th>\n      <th>V321</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>...</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.034990</td>\n      <td>0.004220</td>\n      <td>0.511539</td>\n      <td>0.525111</td>\n      <td>0.406068</td>\n      <td>0.724663</td>\n      <td>0.003008</td>\n      <td>0.002683</td>\n      <td>0.000217</td>\n      <td>0.001816</td>\n      <td>...</td>\n      <td>0.000711</td>\n      <td>0.004432</td>\n      <td>0.005761</td>\n      <td>0.005565</td>\n      <td>0.001172</td>\n      <td>0.001848</td>\n      <td>0.001647</td>\n      <td>0.000177</td>\n      <td>0.000404</td>\n      <td>0.000272</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.183755</td>\n      <td>0.007489</td>\n      <td>0.281741</td>\n      <td>0.313190</td>\n      <td>0.086423</td>\n      <td>0.299967</td>\n      <td>0.028510</td>\n      <td>0.027178</td>\n      <td>0.005790</td>\n      <td>0.030559</td>\n      <td>...</td>\n      <td>0.003122</td>\n      <td>0.019886</td>\n      <td>0.023063</td>\n      <td>0.024230</td>\n      <td>0.024217</td>\n      <td>0.029697</td>\n      <td>0.028365</td>\n      <td>0.003193</td>\n      <td>0.004550</td>\n      <td>0.003671</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.001349</td>\n      <td>0.288515</td>\n      <td>0.230000</td>\n      <td>0.381679</td>\n      <td>0.481752</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.002145</td>\n      <td>0.498850</td>\n      <td>0.522000</td>\n      <td>0.381679</td>\n      <td>0.919708</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.003906</td>\n      <td>0.757875</td>\n      <td>0.824000</td>\n      <td>0.381679</td>\n      <td>0.919708</td>\n      <td>0.000640</td>\n      <td>0.000527</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 107 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "# Scale only numerical columns\n",
    "to_scale = list(set(df_train.select_dtypes(exclude=['object', 'category']).columns) - set(ad_label))\n",
    "print(\"Scaling {} columns\".format(len(to_scale)))\n",
    "df_train[to_scale] = scaler.fit_transform(df_train[to_scale].values.astype(np.float64))\n",
    "df_train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical attributes ProductCD to one-hot encoding\n"
     ]
    },
    {
     "data": {
      "text/plain": "             isFraud  TransactionAmt          card1          card2  \\\ncount  590540.000000   590540.000000  590540.000000  590540.000000   \nmean        0.034990        0.004220       0.511539       0.525111   \nstd         0.183755        0.007489       0.281741       0.313190   \nmin         0.000000        0.000000       0.000000       0.000000   \n25%         0.000000        0.001349       0.288515       0.230000   \n50%         0.000000        0.002145       0.498850       0.522000   \n75%         0.000000        0.003906       0.757875       0.824000   \nmax         1.000000        1.000000       1.000000       1.000000   \n\n               card3          card5             C1             C2  \\\ncount  590540.000000  590540.000000  590540.000000  590540.000000   \nmean        0.406068       0.724663       0.003008       0.002683   \nstd         0.086423       0.299967       0.028510       0.027178   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.381679       0.481752       0.000213       0.000176   \n50%         0.381679       0.919708       0.000213       0.000176   \n75%         0.381679       0.919708       0.000640       0.000527   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n                  C3             C4  ...           V317           V318  \\\ncount  590540.000000  590540.000000  ...  590540.000000  590540.000000   \nmean        0.000217       0.001816  ...       0.001848       0.001647   \nstd         0.005790       0.030559  ...       0.029697       0.028365   \nmin         0.000000       0.000000  ...       0.000000       0.000000   \n25%         0.000000       0.000000  ...       0.000000       0.000000   \n50%         0.000000       0.000000  ...       0.000000       0.000000   \n75%         0.000000       0.000000  ...       0.000000       0.000000   \nmax         1.000000       1.000000  ...       1.000000       1.000000   \n\n                V319           V320           V321    ProductCD_C  \\\ncount  590540.000000  590540.000000  590540.000000  590540.000000   \nmean        0.000177       0.000404       0.000272       0.116028   \nstd         0.003193       0.004550       0.003671       0.320258   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n         ProductCD_H    ProductCD_R    ProductCD_S    ProductCD_W  \ncount  590540.000000  590540.000000  590540.000000  590540.000000  \nmean        0.055922       0.063838       0.019690       0.744522  \nstd         0.229771       0.244465       0.138934       0.436130  \nmin         0.000000       0.000000       0.000000       0.000000  \n25%         0.000000       0.000000       0.000000       0.000000  \n50%         0.000000       0.000000       0.000000       1.000000  \n75%         0.000000       0.000000       0.000000       1.000000  \nmax         1.000000       1.000000       1.000000       1.000000  \n\n[8 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>...</th>\n      <th>V317</th>\n      <th>V318</th>\n      <th>V319</th>\n      <th>V320</th>\n      <th>V321</th>\n      <th>ProductCD_C</th>\n      <th>ProductCD_H</th>\n      <th>ProductCD_R</th>\n      <th>ProductCD_S</th>\n      <th>ProductCD_W</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>...</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n      <td>590540.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.034990</td>\n      <td>0.004220</td>\n      <td>0.511539</td>\n      <td>0.525111</td>\n      <td>0.406068</td>\n      <td>0.724663</td>\n      <td>0.003008</td>\n      <td>0.002683</td>\n      <td>0.000217</td>\n      <td>0.001816</td>\n      <td>...</td>\n      <td>0.001848</td>\n      <td>0.001647</td>\n      <td>0.000177</td>\n      <td>0.000404</td>\n      <td>0.000272</td>\n      <td>0.116028</td>\n      <td>0.055922</td>\n      <td>0.063838</td>\n      <td>0.019690</td>\n      <td>0.744522</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.183755</td>\n      <td>0.007489</td>\n      <td>0.281741</td>\n      <td>0.313190</td>\n      <td>0.086423</td>\n      <td>0.299967</td>\n      <td>0.028510</td>\n      <td>0.027178</td>\n      <td>0.005790</td>\n      <td>0.030559</td>\n      <td>...</td>\n      <td>0.029697</td>\n      <td>0.028365</td>\n      <td>0.003193</td>\n      <td>0.004550</td>\n      <td>0.003671</td>\n      <td>0.320258</td>\n      <td>0.229771</td>\n      <td>0.244465</td>\n      <td>0.138934</td>\n      <td>0.436130</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.001349</td>\n      <td>0.288515</td>\n      <td>0.230000</td>\n      <td>0.381679</td>\n      <td>0.481752</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.002145</td>\n      <td>0.498850</td>\n      <td>0.522000</td>\n      <td>0.381679</td>\n      <td>0.919708</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.003906</td>\n      <td>0.757875</td>\n      <td>0.824000</td>\n      <td>0.381679</td>\n      <td>0.919708</td>\n      <td>0.000640</td>\n      <td>0.000527</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 112 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical values\n",
    "# Before, we save the original column names (will be used when applying the same preprocessing to the test set)\n",
    "train_cols = set(df_train.columns) - set(ad_label)\n",
    "cat_cols = list(set(df_train.select_dtypes(include=['object', 'category']).columns) - set(ad_label))\n",
    "print(\"Converting categorical attributes {} to one-hot encoding\".format(', '.join(cat_cols)))\n",
    "one_hot = pd.get_dummies(df_train[cat_cols])\n",
    "df_train = df_train.drop(columns=cat_cols)\n",
    "df_train = df_train.join(one_hot)\n",
    "df_train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "               isFraud  TransactionAmt     card1     card2     card3  \\\nTransactionID                                                          \n2987000            0.0        0.002137  0.743044  0.525111  0.381679   \n2987001            0.0        0.000900  0.100885  0.608000  0.381679   \n2987002            0.0        0.001840  0.210566  0.780000  0.381679   \n2987003            0.0        0.001558  0.984824  0.934000  0.381679   \n2987004            0.0        0.001558  0.201023  0.828000  0.381679   \n\n                  card5        C1        C2   C3   C4  ...      V317  \\\nTransactionID                                          ...             \n2987000        0.306569  0.000213  0.000176  0.0  0.0  ...  0.000873   \n2987001        0.014599  0.000213  0.000176  0.0  0.0  ...  0.000000   \n2987002        0.481752  0.000213  0.000176  0.0  0.0  ...  0.000000   \n2987003        0.124088  0.000427  0.000879  0.0  0.0  ...  0.010476   \n2987004        0.014599  0.000213  0.000176  0.0  0.0  ...  0.000000   \n\n                   V318  V319  V320  V321  ProductCD_C  ProductCD_H  \\\nTransactionID                                                         \n2987000        0.000000   0.0   0.0   0.0            0            0   \n2987001        0.000000   0.0   0.0   0.0            0            0   \n2987002        0.000000   0.0   0.0   0.0            0            0   \n2987003        0.008022   0.0   0.0   0.0            0            0   \n2987004        0.000000   0.0   0.0   0.0            0            1   \n\n               ProductCD_R  ProductCD_S  ProductCD_W  \nTransactionID                                         \n2987000                  0            0            1  \n2987001                  0            0            1  \n2987002                  0            0            1  \n2987003                  0            0            1  \n2987004                  0            0            0  \n\n[5 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>...</th>\n      <th>V317</th>\n      <th>V318</th>\n      <th>V319</th>\n      <th>V320</th>\n      <th>V321</th>\n      <th>ProductCD_C</th>\n      <th>ProductCD_H</th>\n      <th>ProductCD_R</th>\n      <th>ProductCD_S</th>\n      <th>ProductCD_W</th>\n    </tr>\n    <tr>\n      <th>TransactionID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2987000</th>\n      <td>0.0</td>\n      <td>0.002137</td>\n      <td>0.743044</td>\n      <td>0.525111</td>\n      <td>0.381679</td>\n      <td>0.306569</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000873</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2987001</th>\n      <td>0.0</td>\n      <td>0.000900</td>\n      <td>0.100885</td>\n      <td>0.608000</td>\n      <td>0.381679</td>\n      <td>0.014599</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2987002</th>\n      <td>0.0</td>\n      <td>0.001840</td>\n      <td>0.210566</td>\n      <td>0.780000</td>\n      <td>0.381679</td>\n      <td>0.481752</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2987003</th>\n      <td>0.0</td>\n      <td>0.001558</td>\n      <td>0.984824</td>\n      <td>0.934000</td>\n      <td>0.381679</td>\n      <td>0.124088</td>\n      <td>0.000427</td>\n      <td>0.000879</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.010476</td>\n      <td>0.008022</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2987004</th>\n      <td>0.0</td>\n      <td>0.001558</td>\n      <td>0.201023</td>\n      <td>0.828000</td>\n      <td>0.381679</td>\n      <td>0.014599</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 112 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SMOTE oversampling\n",
    "Our data has a large class imbalance which can lead to overfitting on the majority class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "df_train, train_y = smote.fit_resample(df_train.drop(columns='isFraud'), df_train['isFraud'])\n",
    "df_train = pd.concat((df_train, train_y), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "   TransactionAmt     card1     card2     card3     card5        C1        C2  \\\n0        0.002137  0.743044  0.525111  0.381679  0.306569  0.000213  0.000176   \n1        0.000900  0.100885  0.608000  0.381679  0.014599  0.000213  0.000176   \n2        0.001840  0.210566  0.780000  0.381679  0.481752  0.000213  0.000176   \n3        0.001558  0.984824  0.934000  0.381679  0.124088  0.000427  0.000879   \n4        0.001558  0.201023  0.828000  0.381679  0.014599  0.000213  0.000176   \n\n    C3   C4   C5  ...      V318  V319  V320  V321  ProductCD_C  ProductCD_H  \\\n0  0.0  0.0  0.0  ...  0.000000   0.0   0.0   0.0            0            0   \n1  0.0  0.0  0.0  ...  0.000000   0.0   0.0   0.0            0            0   \n2  0.0  0.0  0.0  ...  0.000000   0.0   0.0   0.0            0            0   \n3  0.0  0.0  0.0  ...  0.008022   0.0   0.0   0.0            0            0   \n4  0.0  0.0  0.0  ...  0.000000   0.0   0.0   0.0            0            1   \n\n   ProductCD_R  ProductCD_S  ProductCD_W  isFraud  \n0            0            0            1      0.0  \n1            0            0            1      0.0  \n2            0            0            1      0.0  \n3            0            0            1      0.0  \n4            0            0            0      0.0  \n\n[5 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>...</th>\n      <th>V318</th>\n      <th>V319</th>\n      <th>V320</th>\n      <th>V321</th>\n      <th>ProductCD_C</th>\n      <th>ProductCD_H</th>\n      <th>ProductCD_R</th>\n      <th>ProductCD_S</th>\n      <th>ProductCD_W</th>\n      <th>isFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.002137</td>\n      <td>0.743044</td>\n      <td>0.525111</td>\n      <td>0.381679</td>\n      <td>0.306569</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000900</td>\n      <td>0.100885</td>\n      <td>0.608000</td>\n      <td>0.381679</td>\n      <td>0.014599</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001840</td>\n      <td>0.210566</td>\n      <td>0.780000</td>\n      <td>0.381679</td>\n      <td>0.481752</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001558</td>\n      <td>0.984824</td>\n      <td>0.934000</td>\n      <td>0.381679</td>\n      <td>0.124088</td>\n      <td>0.000427</td>\n      <td>0.000879</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.008022</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001558</td>\n      <td>0.201023</td>\n      <td>0.828000</td>\n      <td>0.381679</td>\n      <td>0.014599</td>\n      <td>0.000213</td>\n      <td>0.000176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 112 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, save the training set\n",
    "df_train.to_csv(root + \"/processed/clean/train_transaction_clean.csv\", index=False)\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test set preprocessing\n",
    "The steps are the same as for the training set but they can be completed faster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling 106 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat the preceding steps for the test set\n",
    "test_trans_df = pd.read_csv(root + \"/ieee-fraud-detection/test_transaction.csv\", index_col='TransactionID')\n",
    "test_id_df = pd.read_csv(root + \"/ieee-fraud-detection/test_identity.csv\", index_col='TransactionID')\n",
    "df_test = test_trans_df.merge(test_id_df, how='left', left_index=True, right_index=True)\n",
    "df_test.to_csv(root + \"/processed/merged/test_set.csv\", index=True)\n",
    "del test_trans_df\n",
    "del test_id_df\n",
    "\n",
    "# Keep only the columns available during training\n",
    "df_test = df_test[train_cols]\n",
    "df_test.head()\n",
    "\n",
    "# First, deal with missing values\n",
    "cols_missing_vals = df_test.columns[df_test.isna().sum() >= 1]\n",
    "df_test[cols_missing_vals].isna().sum().sort_values(ascending=False)\n",
    "num_nan_cols = df_test[cols_missing_vals].select_dtypes(exclude=['object', 'category']).columns\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_test[num_nan_cols] = imputer.fit_transform(df_test[num_nan_cols].values.astype(np.float64))\n",
    "\n",
    "# Process numerical values\n",
    "num_cols = df_test.select_dtypes(exclude=['object', 'category']).columns\n",
    "scaler = MinMaxScaler()\n",
    "# Scale only numerical columns\n",
    "print(\"Scaling {} columns\".format(len(num_cols)))\n",
    "df_test[num_cols] = scaler.fit_transform(df_test[num_cols].values.astype(np.float64))\n",
    "\n",
    "# Finally, process categorical values\n",
    "cat_cols = df_test.select_dtypes(include=['object', 'category']).columns\n",
    "one_hot = pd.get_dummies(df_test[cat_cols])\n",
    "df_test = df_test.drop(columns=cat_cols)\n",
    "df_test = df_test.join(one_hot)\n",
    "\n",
    "# Finally, save the dataframe\n",
    "df_test.to_csv(root + \"/processed/clean/test_transaction_clean.csv\")\n",
    "df_test.isna().any().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the preprocessing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:/Users/verdi/Documents/Datasets/IEEE-CIS_Fraud_Detection/processed/clean/train_transaction_clean.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/verdi/Documents/Datasets/IEEE-CIS_Fraud_Detection/processed/clean/test_transaction_clean.csv\", index_col=\"TransactionID\")\n",
    "submission = pd.DataFrame(columns=['TransactionID'], data=df_test.index)\n",
    "\n",
    "assert (df_test.isna().any().sum() == 0 and df_train.isna().any().sum() == 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a simple supervised model to test the processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop(columns=[\"isFraud\"], axis=1).to_numpy()\n",
    "X_test = df_test.to_numpy()\n",
    "y_train = df_train[\"isFraud\"].to_numpy()\n",
    "del df_train\n",
    "del df_test\n",
    "X_train.shape[0] == y_train.shape[0] and X_train.shape[1] == X_test.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.208062\n",
      "0:\tlearn: 0.5704404\ttotal: 303ms\tremaining: 5m 2s\n",
      "1:\tlearn: 0.4869556\ttotal: 421ms\tremaining: 3m 30s\n",
      "2:\tlearn: 0.4473522\ttotal: 548ms\tremaining: 3m 2s\n",
      "3:\tlearn: 0.4015736\ttotal: 683ms\tremaining: 2m 49s\n",
      "4:\tlearn: 0.3780410\ttotal: 808ms\tremaining: 2m 40s\n",
      "5:\tlearn: 0.3539398\ttotal: 952ms\tremaining: 2m 37s\n",
      "6:\tlearn: 0.3366261\ttotal: 1.09s\tremaining: 2m 35s\n",
      "7:\tlearn: 0.3171441\ttotal: 1.22s\tremaining: 2m 31s\n",
      "8:\tlearn: 0.3038625\ttotal: 1.35s\tremaining: 2m 29s\n",
      "9:\tlearn: 0.2966965\ttotal: 1.47s\tremaining: 2m 25s\n",
      "10:\tlearn: 0.2866670\ttotal: 1.58s\tremaining: 2m 21s\n",
      "11:\tlearn: 0.2711913\ttotal: 1.72s\tremaining: 2m 22s\n",
      "12:\tlearn: 0.2673570\ttotal: 1.83s\tremaining: 2m 19s\n",
      "13:\tlearn: 0.2550893\ttotal: 1.96s\tremaining: 2m 17s\n",
      "14:\tlearn: 0.2503645\ttotal: 2.07s\tremaining: 2m 15s\n",
      "15:\tlearn: 0.2463506\ttotal: 2.2s\tremaining: 2m 15s\n",
      "16:\tlearn: 0.2410873\ttotal: 2.33s\tremaining: 2m 15s\n",
      "17:\tlearn: 0.2369392\ttotal: 2.47s\tremaining: 2m 14s\n",
      "18:\tlearn: 0.2338993\ttotal: 2.58s\tremaining: 2m 13s\n",
      "19:\tlearn: 0.2224003\ttotal: 2.7s\tremaining: 2m 12s\n",
      "20:\tlearn: 0.2147188\ttotal: 2.83s\tremaining: 2m 11s\n",
      "21:\tlearn: 0.2060112\ttotal: 2.94s\tremaining: 2m 10s\n",
      "22:\tlearn: 0.2026785\ttotal: 3.06s\tremaining: 2m 9s\n",
      "23:\tlearn: 0.2004086\ttotal: 3.18s\tremaining: 2m 9s\n",
      "24:\tlearn: 0.1953424\ttotal: 3.33s\tremaining: 2m 10s\n",
      "25:\tlearn: 0.1935798\ttotal: 3.46s\tremaining: 2m 9s\n",
      "26:\tlearn: 0.1912062\ttotal: 3.58s\tremaining: 2m 9s\n",
      "27:\tlearn: 0.1879609\ttotal: 3.71s\tremaining: 2m 8s\n",
      "28:\tlearn: 0.1862544\ttotal: 3.85s\tremaining: 2m 8s\n",
      "29:\tlearn: 0.1848034\ttotal: 3.96s\tremaining: 2m 8s\n",
      "30:\tlearn: 0.1830469\ttotal: 4.07s\tremaining: 2m 7s\n",
      "31:\tlearn: 0.1800459\ttotal: 4.19s\tremaining: 2m 6s\n",
      "32:\tlearn: 0.1783834\ttotal: 4.31s\tremaining: 2m 6s\n",
      "33:\tlearn: 0.1774079\ttotal: 4.44s\tremaining: 2m 6s\n",
      "34:\tlearn: 0.1759223\ttotal: 4.56s\tremaining: 2m 5s\n",
      "35:\tlearn: 0.1734232\ttotal: 4.67s\tremaining: 2m 5s\n",
      "36:\tlearn: 0.1707491\ttotal: 4.81s\tremaining: 2m 5s\n",
      "37:\tlearn: 0.1696500\ttotal: 4.92s\tremaining: 2m 4s\n",
      "38:\tlearn: 0.1684690\ttotal: 5.04s\tremaining: 2m 4s\n",
      "39:\tlearn: 0.1654054\ttotal: 5.16s\tremaining: 2m 3s\n",
      "40:\tlearn: 0.1644453\ttotal: 5.27s\tremaining: 2m 3s\n",
      "41:\tlearn: 0.1635891\ttotal: 5.38s\tremaining: 2m 2s\n",
      "42:\tlearn: 0.1610133\ttotal: 5.51s\tremaining: 2m 2s\n",
      "43:\tlearn: 0.1571453\ttotal: 5.64s\tremaining: 2m 2s\n",
      "44:\tlearn: 0.1551877\ttotal: 5.75s\tremaining: 2m 1s\n",
      "45:\tlearn: 0.1532975\ttotal: 5.86s\tremaining: 2m 1s\n",
      "46:\tlearn: 0.1518780\ttotal: 5.97s\tremaining: 2m 1s\n",
      "47:\tlearn: 0.1509252\ttotal: 6.09s\tremaining: 2m\n",
      "48:\tlearn: 0.1500574\ttotal: 6.21s\tremaining: 2m\n",
      "49:\tlearn: 0.1497751\ttotal: 6.31s\tremaining: 1m 59s\n",
      "50:\tlearn: 0.1489525\ttotal: 6.42s\tremaining: 1m 59s\n",
      "51:\tlearn: 0.1481181\ttotal: 6.55s\tremaining: 1m 59s\n",
      "52:\tlearn: 0.1468822\ttotal: 6.69s\tremaining: 1m 59s\n",
      "53:\tlearn: 0.1464024\ttotal: 6.81s\tremaining: 1m 59s\n",
      "54:\tlearn: 0.1452443\ttotal: 6.95s\tremaining: 1m 59s\n",
      "55:\tlearn: 0.1446123\ttotal: 7.08s\tremaining: 1m 59s\n",
      "56:\tlearn: 0.1435978\ttotal: 7.21s\tremaining: 1m 59s\n",
      "57:\tlearn: 0.1418121\ttotal: 7.32s\tremaining: 1m 58s\n",
      "58:\tlearn: 0.1406981\ttotal: 7.44s\tremaining: 1m 58s\n",
      "59:\tlearn: 0.1376202\ttotal: 7.57s\tremaining: 1m 58s\n",
      "60:\tlearn: 0.1371613\ttotal: 7.69s\tremaining: 1m 58s\n",
      "61:\tlearn: 0.1368398\ttotal: 7.8s\tremaining: 1m 57s\n",
      "62:\tlearn: 0.1340518\ttotal: 7.93s\tremaining: 1m 57s\n",
      "63:\tlearn: 0.1334497\ttotal: 8.05s\tremaining: 1m 57s\n",
      "64:\tlearn: 0.1312957\ttotal: 8.16s\tremaining: 1m 57s\n",
      "65:\tlearn: 0.1310435\ttotal: 8.26s\tremaining: 1m 56s\n",
      "66:\tlearn: 0.1304925\ttotal: 8.38s\tremaining: 1m 56s\n",
      "67:\tlearn: 0.1299241\ttotal: 8.53s\tremaining: 1m 56s\n",
      "68:\tlearn: 0.1289947\ttotal: 8.64s\tremaining: 1m 56s\n",
      "69:\tlearn: 0.1272567\ttotal: 8.77s\tremaining: 1m 56s\n",
      "70:\tlearn: 0.1263917\ttotal: 8.89s\tremaining: 1m 56s\n",
      "71:\tlearn: 0.1259102\ttotal: 9s\tremaining: 1m 56s\n",
      "72:\tlearn: 0.1254220\ttotal: 9.12s\tremaining: 1m 55s\n",
      "73:\tlearn: 0.1247279\ttotal: 9.24s\tremaining: 1m 55s\n",
      "74:\tlearn: 0.1241615\ttotal: 9.36s\tremaining: 1m 55s\n",
      "75:\tlearn: 0.1231592\ttotal: 9.5s\tremaining: 1m 55s\n",
      "76:\tlearn: 0.1222230\ttotal: 9.62s\tremaining: 1m 55s\n",
      "77:\tlearn: 0.1216636\ttotal: 9.73s\tremaining: 1m 54s\n",
      "78:\tlearn: 0.1200976\ttotal: 9.86s\tremaining: 1m 54s\n",
      "79:\tlearn: 0.1191560\ttotal: 10s\tremaining: 1m 55s\n",
      "80:\tlearn: 0.1188508\ttotal: 10.1s\tremaining: 1m 54s\n",
      "81:\tlearn: 0.1183546\ttotal: 10.2s\tremaining: 1m 54s\n",
      "82:\tlearn: 0.1179000\ttotal: 10.4s\tremaining: 1m 54s\n",
      "83:\tlearn: 0.1175125\ttotal: 10.5s\tremaining: 1m 54s\n",
      "84:\tlearn: 0.1172233\ttotal: 10.6s\tremaining: 1m 54s\n",
      "85:\tlearn: 0.1169488\ttotal: 10.7s\tremaining: 1m 53s\n",
      "86:\tlearn: 0.1166168\ttotal: 10.8s\tremaining: 1m 53s\n",
      "87:\tlearn: 0.1162420\ttotal: 10.9s\tremaining: 1m 53s\n",
      "88:\tlearn: 0.1160595\ttotal: 11s\tremaining: 1m 53s\n",
      "89:\tlearn: 0.1158724\ttotal: 11.2s\tremaining: 1m 52s\n",
      "90:\tlearn: 0.1155870\ttotal: 11.3s\tremaining: 1m 52s\n",
      "91:\tlearn: 0.1152553\ttotal: 11.4s\tremaining: 1m 52s\n",
      "92:\tlearn: 0.1146562\ttotal: 11.5s\tremaining: 1m 52s\n",
      "93:\tlearn: 0.1144243\ttotal: 11.6s\tremaining: 1m 51s\n",
      "94:\tlearn: 0.1139984\ttotal: 11.7s\tremaining: 1m 51s\n",
      "95:\tlearn: 0.1130790\ttotal: 11.8s\tremaining: 1m 51s\n",
      "96:\tlearn: 0.1128695\ttotal: 11.9s\tremaining: 1m 51s\n",
      "97:\tlearn: 0.1124675\ttotal: 12s\tremaining: 1m 50s\n",
      "98:\tlearn: 0.1118109\ttotal: 12.1s\tremaining: 1m 50s\n",
      "99:\tlearn: 0.1115426\ttotal: 12.3s\tremaining: 1m 50s\n",
      "100:\tlearn: 0.1113608\ttotal: 12.4s\tremaining: 1m 50s\n",
      "101:\tlearn: 0.1109914\ttotal: 12.5s\tremaining: 1m 49s\n",
      "102:\tlearn: 0.1097465\ttotal: 12.6s\tremaining: 1m 49s\n",
      "103:\tlearn: 0.1094735\ttotal: 12.7s\tremaining: 1m 49s\n",
      "104:\tlearn: 0.1092712\ttotal: 12.8s\tremaining: 1m 49s\n",
      "105:\tlearn: 0.1090800\ttotal: 12.9s\tremaining: 1m 49s\n",
      "106:\tlearn: 0.1089201\ttotal: 13.1s\tremaining: 1m 49s\n",
      "107:\tlearn: 0.1086425\ttotal: 13.2s\tremaining: 1m 48s\n",
      "108:\tlearn: 0.1084255\ttotal: 13.3s\tremaining: 1m 48s\n",
      "109:\tlearn: 0.1081486\ttotal: 13.4s\tremaining: 1m 48s\n",
      "110:\tlearn: 0.1079449\ttotal: 13.5s\tremaining: 1m 48s\n",
      "111:\tlearn: 0.1075958\ttotal: 13.7s\tremaining: 1m 48s\n",
      "112:\tlearn: 0.1071626\ttotal: 13.8s\tremaining: 1m 48s\n",
      "113:\tlearn: 0.1070252\ttotal: 13.9s\tremaining: 1m 47s\n",
      "114:\tlearn: 0.1067700\ttotal: 14s\tremaining: 1m 47s\n",
      "115:\tlearn: 0.1065785\ttotal: 14.1s\tremaining: 1m 47s\n",
      "116:\tlearn: 0.1064328\ttotal: 14.2s\tremaining: 1m 47s\n",
      "117:\tlearn: 0.1062124\ttotal: 14.4s\tremaining: 1m 47s\n",
      "118:\tlearn: 0.1060246\ttotal: 14.5s\tremaining: 1m 47s\n",
      "119:\tlearn: 0.1059112\ttotal: 14.6s\tremaining: 1m 46s\n",
      "120:\tlearn: 0.1056941\ttotal: 14.7s\tremaining: 1m 46s\n",
      "121:\tlearn: 0.1055666\ttotal: 14.8s\tremaining: 1m 46s\n",
      "122:\tlearn: 0.1053796\ttotal: 14.9s\tremaining: 1m 46s\n",
      "123:\tlearn: 0.1050739\ttotal: 15s\tremaining: 1m 46s\n",
      "124:\tlearn: 0.1049028\ttotal: 15.2s\tremaining: 1m 46s\n",
      "125:\tlearn: 0.1045380\ttotal: 15.3s\tremaining: 1m 45s\n",
      "126:\tlearn: 0.1042131\ttotal: 15.4s\tremaining: 1m 45s\n",
      "127:\tlearn: 0.1038946\ttotal: 15.5s\tremaining: 1m 45s\n",
      "128:\tlearn: 0.1036220\ttotal: 15.7s\tremaining: 1m 45s\n",
      "129:\tlearn: 0.1033800\ttotal: 15.8s\tremaining: 1m 45s\n",
      "130:\tlearn: 0.1031948\ttotal: 15.9s\tremaining: 1m 45s\n",
      "131:\tlearn: 0.1030263\ttotal: 16s\tremaining: 1m 45s\n",
      "132:\tlearn: 0.1029131\ttotal: 16.1s\tremaining: 1m 45s\n",
      "133:\tlearn: 0.1027403\ttotal: 16.2s\tremaining: 1m 45s\n",
      "134:\tlearn: 0.1024746\ttotal: 16.4s\tremaining: 1m 44s\n",
      "135:\tlearn: 0.1022800\ttotal: 16.5s\tremaining: 1m 44s\n",
      "136:\tlearn: 0.1018446\ttotal: 16.6s\tremaining: 1m 44s\n",
      "137:\tlearn: 0.1016694\ttotal: 16.7s\tremaining: 1m 44s\n",
      "138:\tlearn: 0.1012719\ttotal: 16.9s\tremaining: 1m 44s\n",
      "139:\tlearn: 0.1011653\ttotal: 17s\tremaining: 1m 44s\n",
      "140:\tlearn: 0.1010081\ttotal: 17.1s\tremaining: 1m 44s\n",
      "141:\tlearn: 0.1009281\ttotal: 17.2s\tremaining: 1m 44s\n",
      "142:\tlearn: 0.1007785\ttotal: 17.4s\tremaining: 1m 44s\n",
      "143:\tlearn: 0.1006349\ttotal: 17.5s\tremaining: 1m 43s\n",
      "144:\tlearn: 0.1003960\ttotal: 17.6s\tremaining: 1m 43s\n",
      "145:\tlearn: 0.1002483\ttotal: 17.7s\tremaining: 1m 43s\n",
      "146:\tlearn: 0.1000776\ttotal: 17.9s\tremaining: 1m 43s\n",
      "147:\tlearn: 0.0998706\ttotal: 18s\tremaining: 1m 43s\n",
      "148:\tlearn: 0.0995653\ttotal: 18.1s\tremaining: 1m 43s\n",
      "149:\tlearn: 0.0994691\ttotal: 18.2s\tremaining: 1m 43s\n",
      "150:\tlearn: 0.0994065\ttotal: 18.3s\tremaining: 1m 42s\n",
      "151:\tlearn: 0.0992425\ttotal: 18.4s\tremaining: 1m 42s\n",
      "152:\tlearn: 0.0990767\ttotal: 18.6s\tremaining: 1m 42s\n",
      "153:\tlearn: 0.0989611\ttotal: 18.7s\tremaining: 1m 42s\n",
      "154:\tlearn: 0.0987681\ttotal: 18.8s\tremaining: 1m 42s\n",
      "155:\tlearn: 0.0986283\ttotal: 18.9s\tremaining: 1m 42s\n",
      "156:\tlearn: 0.0985848\ttotal: 19s\tremaining: 1m 42s\n",
      "157:\tlearn: 0.0984413\ttotal: 19.1s\tremaining: 1m 42s\n",
      "158:\tlearn: 0.0983695\ttotal: 19.3s\tremaining: 1m 41s\n",
      "159:\tlearn: 0.0982037\ttotal: 19.4s\tremaining: 1m 41s\n",
      "160:\tlearn: 0.0980529\ttotal: 19.5s\tremaining: 1m 41s\n",
      "161:\tlearn: 0.0978735\ttotal: 19.6s\tremaining: 1m 41s\n",
      "162:\tlearn: 0.0976479\ttotal: 19.7s\tremaining: 1m 41s\n",
      "163:\tlearn: 0.0975524\ttotal: 19.8s\tremaining: 1m 41s\n",
      "164:\tlearn: 0.0974326\ttotal: 19.9s\tremaining: 1m 40s\n",
      "165:\tlearn: 0.0972961\ttotal: 20.1s\tremaining: 1m 40s\n",
      "166:\tlearn: 0.0971356\ttotal: 20.2s\tremaining: 1m 40s\n",
      "167:\tlearn: 0.0970718\ttotal: 20.3s\tremaining: 1m 40s\n",
      "168:\tlearn: 0.0966324\ttotal: 20.4s\tremaining: 1m 40s\n",
      "169:\tlearn: 0.0964381\ttotal: 20.5s\tremaining: 1m 40s\n",
      "170:\tlearn: 0.0963104\ttotal: 20.7s\tremaining: 1m 40s\n",
      "171:\tlearn: 0.0958430\ttotal: 20.8s\tremaining: 1m 40s\n",
      "172:\tlearn: 0.0952827\ttotal: 20.9s\tremaining: 1m 40s\n",
      "173:\tlearn: 0.0951758\ttotal: 21.1s\tremaining: 1m 39s\n",
      "174:\tlearn: 0.0950693\ttotal: 21.2s\tremaining: 1m 39s\n",
      "175:\tlearn: 0.0949540\ttotal: 21.3s\tremaining: 1m 39s\n",
      "176:\tlearn: 0.0948293\ttotal: 21.4s\tremaining: 1m 39s\n",
      "177:\tlearn: 0.0947498\ttotal: 21.5s\tremaining: 1m 39s\n",
      "178:\tlearn: 0.0946247\ttotal: 21.6s\tremaining: 1m 39s\n",
      "179:\tlearn: 0.0945464\ttotal: 21.8s\tremaining: 1m 39s\n",
      "180:\tlearn: 0.0943592\ttotal: 21.9s\tremaining: 1m 38s\n",
      "181:\tlearn: 0.0942531\ttotal: 22s\tremaining: 1m 38s\n",
      "182:\tlearn: 0.0940973\ttotal: 22.1s\tremaining: 1m 38s\n",
      "183:\tlearn: 0.0940465\ttotal: 22.2s\tremaining: 1m 38s\n",
      "184:\tlearn: 0.0938585\ttotal: 22.3s\tremaining: 1m 38s\n",
      "185:\tlearn: 0.0937416\ttotal: 22.4s\tremaining: 1m 38s\n",
      "186:\tlearn: 0.0936356\ttotal: 22.5s\tremaining: 1m 37s\n",
      "187:\tlearn: 0.0935293\ttotal: 22.7s\tremaining: 1m 37s\n",
      "188:\tlearn: 0.0933763\ttotal: 22.8s\tremaining: 1m 37s\n",
      "189:\tlearn: 0.0932640\ttotal: 22.9s\tremaining: 1m 37s\n",
      "190:\tlearn: 0.0931744\ttotal: 23s\tremaining: 1m 37s\n",
      "191:\tlearn: 0.0930461\ttotal: 23.1s\tremaining: 1m 37s\n",
      "192:\tlearn: 0.0929639\ttotal: 23.2s\tremaining: 1m 37s\n",
      "193:\tlearn: 0.0928649\ttotal: 23.4s\tremaining: 1m 37s\n",
      "194:\tlearn: 0.0925514\ttotal: 23.5s\tremaining: 1m 36s\n",
      "195:\tlearn: 0.0924222\ttotal: 23.6s\tremaining: 1m 36s\n",
      "196:\tlearn: 0.0923169\ttotal: 23.7s\tremaining: 1m 36s\n",
      "197:\tlearn: 0.0921867\ttotal: 23.8s\tremaining: 1m 36s\n",
      "198:\tlearn: 0.0920620\ttotal: 23.9s\tremaining: 1m 36s\n",
      "199:\tlearn: 0.0919795\ttotal: 24s\tremaining: 1m 36s\n",
      "200:\tlearn: 0.0918805\ttotal: 24.2s\tremaining: 1m 36s\n",
      "201:\tlearn: 0.0917973\ttotal: 24.3s\tremaining: 1m 35s\n",
      "202:\tlearn: 0.0916622\ttotal: 24.4s\tremaining: 1m 35s\n",
      "203:\tlearn: 0.0914611\ttotal: 24.5s\tremaining: 1m 35s\n",
      "204:\tlearn: 0.0913625\ttotal: 24.6s\tremaining: 1m 35s\n",
      "205:\tlearn: 0.0912504\ttotal: 24.8s\tremaining: 1m 35s\n",
      "206:\tlearn: 0.0911767\ttotal: 24.9s\tremaining: 1m 35s\n",
      "207:\tlearn: 0.0910948\ttotal: 25s\tremaining: 1m 35s\n",
      "208:\tlearn: 0.0909690\ttotal: 25.1s\tremaining: 1m 34s\n",
      "209:\tlearn: 0.0909309\ttotal: 25.2s\tremaining: 1m 34s\n",
      "210:\tlearn: 0.0908136\ttotal: 25.3s\tremaining: 1m 34s\n",
      "211:\tlearn: 0.0907125\ttotal: 25.5s\tremaining: 1m 34s\n",
      "212:\tlearn: 0.0906001\ttotal: 25.6s\tremaining: 1m 34s\n",
      "213:\tlearn: 0.0905165\ttotal: 25.7s\tremaining: 1m 34s\n",
      "214:\tlearn: 0.0903907\ttotal: 25.8s\tremaining: 1m 34s\n",
      "215:\tlearn: 0.0902063\ttotal: 25.9s\tremaining: 1m 34s\n",
      "216:\tlearn: 0.0901112\ttotal: 26s\tremaining: 1m 33s\n",
      "217:\tlearn: 0.0900397\ttotal: 26.1s\tremaining: 1m 33s\n",
      "218:\tlearn: 0.0899647\ttotal: 26.2s\tremaining: 1m 33s\n",
      "219:\tlearn: 0.0898821\ttotal: 26.4s\tremaining: 1m 33s\n",
      "220:\tlearn: 0.0897829\ttotal: 26.5s\tremaining: 1m 33s\n",
      "221:\tlearn: 0.0896692\ttotal: 26.6s\tremaining: 1m 33s\n",
      "222:\tlearn: 0.0894433\ttotal: 26.7s\tremaining: 1m 33s\n",
      "223:\tlearn: 0.0893396\ttotal: 26.9s\tremaining: 1m 33s\n",
      "224:\tlearn: 0.0891998\ttotal: 27s\tremaining: 1m 33s\n",
      "225:\tlearn: 0.0891336\ttotal: 27.1s\tremaining: 1m 32s\n",
      "226:\tlearn: 0.0890873\ttotal: 27.2s\tremaining: 1m 32s\n",
      "227:\tlearn: 0.0886907\ttotal: 27.4s\tremaining: 1m 32s\n",
      "228:\tlearn: 0.0886020\ttotal: 27.5s\tremaining: 1m 32s\n",
      "229:\tlearn: 0.0885498\ttotal: 27.6s\tremaining: 1m 32s\n",
      "230:\tlearn: 0.0884545\ttotal: 27.7s\tremaining: 1m 32s\n",
      "231:\tlearn: 0.0882946\ttotal: 27.8s\tremaining: 1m 32s\n",
      "232:\tlearn: 0.0880147\ttotal: 28s\tremaining: 1m 32s\n",
      "233:\tlearn: 0.0878344\ttotal: 28.1s\tremaining: 1m 32s\n",
      "234:\tlearn: 0.0877484\ttotal: 28.2s\tremaining: 1m 31s\n",
      "235:\tlearn: 0.0876866\ttotal: 28.3s\tremaining: 1m 31s\n",
      "236:\tlearn: 0.0875077\ttotal: 28.5s\tremaining: 1m 31s\n",
      "237:\tlearn: 0.0874573\ttotal: 28.6s\tremaining: 1m 31s\n",
      "238:\tlearn: 0.0873711\ttotal: 28.7s\tremaining: 1m 31s\n",
      "239:\tlearn: 0.0872347\ttotal: 28.9s\tremaining: 1m 31s\n",
      "240:\tlearn: 0.0871265\ttotal: 29s\tremaining: 1m 31s\n",
      "241:\tlearn: 0.0870047\ttotal: 29.1s\tremaining: 1m 31s\n",
      "242:\tlearn: 0.0869518\ttotal: 29.2s\tremaining: 1m 31s\n",
      "243:\tlearn: 0.0868672\ttotal: 29.4s\tremaining: 1m 30s\n",
      "244:\tlearn: 0.0867762\ttotal: 29.5s\tremaining: 1m 30s\n",
      "245:\tlearn: 0.0866433\ttotal: 29.6s\tremaining: 1m 30s\n",
      "246:\tlearn: 0.0863629\ttotal: 29.7s\tremaining: 1m 30s\n",
      "247:\tlearn: 0.0861374\ttotal: 29.8s\tremaining: 1m 30s\n",
      "248:\tlearn: 0.0860935\ttotal: 30s\tremaining: 1m 30s\n",
      "249:\tlearn: 0.0860169\ttotal: 30.1s\tremaining: 1m 30s\n",
      "250:\tlearn: 0.0859314\ttotal: 30.2s\tremaining: 1m 30s\n",
      "251:\tlearn: 0.0858623\ttotal: 30.3s\tremaining: 1m 30s\n",
      "252:\tlearn: 0.0857902\ttotal: 30.5s\tremaining: 1m 29s\n",
      "253:\tlearn: 0.0857313\ttotal: 30.6s\tremaining: 1m 29s\n",
      "254:\tlearn: 0.0856467\ttotal: 30.7s\tremaining: 1m 29s\n",
      "255:\tlearn: 0.0855451\ttotal: 30.8s\tremaining: 1m 29s\n",
      "256:\tlearn: 0.0854466\ttotal: 31s\tremaining: 1m 29s\n",
      "257:\tlearn: 0.0854066\ttotal: 31.1s\tremaining: 1m 29s\n",
      "258:\tlearn: 0.0852832\ttotal: 31.2s\tremaining: 1m 29s\n",
      "259:\tlearn: 0.0852210\ttotal: 31.4s\tremaining: 1m 29s\n",
      "260:\tlearn: 0.0851513\ttotal: 31.5s\tremaining: 1m 29s\n",
      "261:\tlearn: 0.0850966\ttotal: 31.6s\tremaining: 1m 28s\n",
      "262:\tlearn: 0.0850442\ttotal: 31.7s\tremaining: 1m 28s\n",
      "263:\tlearn: 0.0849105\ttotal: 31.8s\tremaining: 1m 28s\n",
      "264:\tlearn: 0.0848028\ttotal: 31.9s\tremaining: 1m 28s\n",
      "265:\tlearn: 0.0846938\ttotal: 32.1s\tremaining: 1m 28s\n",
      "266:\tlearn: 0.0846379\ttotal: 32.2s\tremaining: 1m 28s\n",
      "267:\tlearn: 0.0845645\ttotal: 32.3s\tremaining: 1m 28s\n",
      "268:\tlearn: 0.0844806\ttotal: 32.4s\tremaining: 1m 28s\n",
      "269:\tlearn: 0.0844487\ttotal: 32.5s\tremaining: 1m 27s\n",
      "270:\tlearn: 0.0843934\ttotal: 32.7s\tremaining: 1m 27s\n",
      "271:\tlearn: 0.0843561\ttotal: 32.8s\tremaining: 1m 27s\n",
      "272:\tlearn: 0.0842793\ttotal: 32.9s\tremaining: 1m 27s\n",
      "273:\tlearn: 0.0842233\ttotal: 33s\tremaining: 1m 27s\n",
      "274:\tlearn: 0.0841604\ttotal: 33.1s\tremaining: 1m 27s\n",
      "275:\tlearn: 0.0841379\ttotal: 33.2s\tremaining: 1m 27s\n",
      "276:\tlearn: 0.0841070\ttotal: 33.3s\tremaining: 1m 27s\n",
      "277:\tlearn: 0.0840428\ttotal: 33.5s\tremaining: 1m 26s\n",
      "278:\tlearn: 0.0839595\ttotal: 33.6s\tremaining: 1m 26s\n",
      "279:\tlearn: 0.0838831\ttotal: 33.7s\tremaining: 1m 26s\n",
      "280:\tlearn: 0.0838065\ttotal: 33.8s\tremaining: 1m 26s\n",
      "281:\tlearn: 0.0837298\ttotal: 34s\tremaining: 1m 26s\n",
      "282:\tlearn: 0.0835084\ttotal: 34.1s\tremaining: 1m 26s\n",
      "283:\tlearn: 0.0834627\ttotal: 34.2s\tremaining: 1m 26s\n",
      "284:\tlearn: 0.0833995\ttotal: 34.3s\tremaining: 1m 26s\n",
      "285:\tlearn: 0.0833364\ttotal: 34.5s\tremaining: 1m 26s\n",
      "286:\tlearn: 0.0831335\ttotal: 34.6s\tremaining: 1m 25s\n",
      "287:\tlearn: 0.0830640\ttotal: 34.7s\tremaining: 1m 25s\n",
      "288:\tlearn: 0.0830089\ttotal: 34.8s\tremaining: 1m 25s\n",
      "289:\tlearn: 0.0829809\ttotal: 34.9s\tremaining: 1m 25s\n",
      "290:\tlearn: 0.0828773\ttotal: 35s\tremaining: 1m 25s\n",
      "291:\tlearn: 0.0828400\ttotal: 35.2s\tremaining: 1m 25s\n",
      "292:\tlearn: 0.0827510\ttotal: 35.3s\tremaining: 1m 25s\n",
      "293:\tlearn: 0.0826631\ttotal: 35.4s\tremaining: 1m 25s\n",
      "294:\tlearn: 0.0826171\ttotal: 35.5s\tremaining: 1m 24s\n",
      "295:\tlearn: 0.0825702\ttotal: 35.6s\tremaining: 1m 24s\n",
      "296:\tlearn: 0.0825112\ttotal: 35.8s\tremaining: 1m 24s\n",
      "297:\tlearn: 0.0824386\ttotal: 35.9s\tremaining: 1m 24s\n",
      "298:\tlearn: 0.0823805\ttotal: 36s\tremaining: 1m 24s\n",
      "299:\tlearn: 0.0823157\ttotal: 36.1s\tremaining: 1m 24s\n",
      "300:\tlearn: 0.0822210\ttotal: 36.2s\tremaining: 1m 24s\n",
      "301:\tlearn: 0.0822037\ttotal: 36.3s\tremaining: 1m 23s\n",
      "302:\tlearn: 0.0821359\ttotal: 36.5s\tremaining: 1m 23s\n",
      "303:\tlearn: 0.0820391\ttotal: 36.6s\tremaining: 1m 23s\n",
      "304:\tlearn: 0.0819935\ttotal: 36.7s\tremaining: 1m 23s\n",
      "305:\tlearn: 0.0818988\ttotal: 36.8s\tremaining: 1m 23s\n",
      "306:\tlearn: 0.0818603\ttotal: 36.9s\tremaining: 1m 23s\n",
      "307:\tlearn: 0.0818190\ttotal: 37s\tremaining: 1m 23s\n",
      "308:\tlearn: 0.0817952\ttotal: 37.1s\tremaining: 1m 23s\n",
      "309:\tlearn: 0.0817509\ttotal: 37.3s\tremaining: 1m 22s\n",
      "310:\tlearn: 0.0817133\ttotal: 37.4s\tremaining: 1m 22s\n",
      "311:\tlearn: 0.0816632\ttotal: 37.5s\tremaining: 1m 22s\n",
      "312:\tlearn: 0.0815908\ttotal: 37.6s\tremaining: 1m 22s\n",
      "313:\tlearn: 0.0815291\ttotal: 37.7s\tremaining: 1m 22s\n",
      "314:\tlearn: 0.0814918\ttotal: 37.8s\tremaining: 1m 22s\n",
      "315:\tlearn: 0.0814430\ttotal: 38s\tremaining: 1m 22s\n",
      "316:\tlearn: 0.0814073\ttotal: 38.1s\tremaining: 1m 22s\n",
      "317:\tlearn: 0.0813843\ttotal: 38.2s\tremaining: 1m 21s\n",
      "318:\tlearn: 0.0813498\ttotal: 38.3s\tremaining: 1m 21s\n",
      "319:\tlearn: 0.0812969\ttotal: 38.4s\tremaining: 1m 21s\n",
      "320:\tlearn: 0.0812129\ttotal: 38.5s\tremaining: 1m 21s\n",
      "321:\tlearn: 0.0811595\ttotal: 38.6s\tremaining: 1m 21s\n",
      "322:\tlearn: 0.0809812\ttotal: 38.8s\tremaining: 1m 21s\n",
      "323:\tlearn: 0.0807836\ttotal: 38.9s\tremaining: 1m 21s\n",
      "324:\tlearn: 0.0807141\ttotal: 39s\tremaining: 1m 20s\n",
      "325:\tlearn: 0.0806786\ttotal: 39.1s\tremaining: 1m 20s\n",
      "326:\tlearn: 0.0805790\ttotal: 39.2s\tremaining: 1m 20s\n",
      "327:\tlearn: 0.0805533\ttotal: 39.3s\tremaining: 1m 20s\n",
      "328:\tlearn: 0.0805158\ttotal: 39.4s\tremaining: 1m 20s\n",
      "329:\tlearn: 0.0804922\ttotal: 39.5s\tremaining: 1m 20s\n",
      "330:\tlearn: 0.0804116\ttotal: 39.7s\tremaining: 1m 20s\n",
      "331:\tlearn: 0.0803496\ttotal: 39.8s\tremaining: 1m 20s\n",
      "332:\tlearn: 0.0802899\ttotal: 39.9s\tremaining: 1m 19s\n",
      "333:\tlearn: 0.0802328\ttotal: 40s\tremaining: 1m 19s\n",
      "334:\tlearn: 0.0801472\ttotal: 40.2s\tremaining: 1m 19s\n",
      "335:\tlearn: 0.0800607\ttotal: 40.3s\tremaining: 1m 19s\n",
      "336:\tlearn: 0.0799694\ttotal: 40.4s\tremaining: 1m 19s\n",
      "337:\tlearn: 0.0799193\ttotal: 40.5s\tremaining: 1m 19s\n",
      "338:\tlearn: 0.0798784\ttotal: 40.7s\tremaining: 1m 19s\n",
      "339:\tlearn: 0.0797967\ttotal: 40.8s\tremaining: 1m 19s\n",
      "340:\tlearn: 0.0797568\ttotal: 40.9s\tremaining: 1m 19s\n",
      "341:\tlearn: 0.0797333\ttotal: 41s\tremaining: 1m 18s\n",
      "342:\tlearn: 0.0796937\ttotal: 41.1s\tremaining: 1m 18s\n",
      "343:\tlearn: 0.0796515\ttotal: 41.2s\tremaining: 1m 18s\n",
      "344:\tlearn: 0.0796103\ttotal: 41.4s\tremaining: 1m 18s\n",
      "345:\tlearn: 0.0796005\ttotal: 41.5s\tremaining: 1m 18s\n",
      "346:\tlearn: 0.0795362\ttotal: 41.6s\tremaining: 1m 18s\n",
      "347:\tlearn: 0.0794747\ttotal: 41.8s\tremaining: 1m 18s\n",
      "348:\tlearn: 0.0793980\ttotal: 41.9s\tremaining: 1m 18s\n",
      "349:\tlearn: 0.0793568\ttotal: 42s\tremaining: 1m 17s\n",
      "350:\tlearn: 0.0793152\ttotal: 42.1s\tremaining: 1m 17s\n",
      "351:\tlearn: 0.0792534\ttotal: 42.2s\tremaining: 1m 17s\n",
      "352:\tlearn: 0.0792071\ttotal: 42.4s\tremaining: 1m 17s\n",
      "353:\tlearn: 0.0791514\ttotal: 42.5s\tremaining: 1m 17s\n",
      "354:\tlearn: 0.0791128\ttotal: 42.6s\tremaining: 1m 17s\n",
      "355:\tlearn: 0.0790769\ttotal: 42.7s\tremaining: 1m 17s\n",
      "356:\tlearn: 0.0790377\ttotal: 42.8s\tremaining: 1m 17s\n",
      "357:\tlearn: 0.0789621\ttotal: 43s\tremaining: 1m 17s\n",
      "358:\tlearn: 0.0789304\ttotal: 43.1s\tremaining: 1m 16s\n",
      "359:\tlearn: 0.0788804\ttotal: 43.2s\tremaining: 1m 16s\n",
      "360:\tlearn: 0.0788182\ttotal: 43.3s\tremaining: 1m 16s\n",
      "361:\tlearn: 0.0787739\ttotal: 43.4s\tremaining: 1m 16s\n",
      "362:\tlearn: 0.0787193\ttotal: 43.6s\tremaining: 1m 16s\n",
      "363:\tlearn: 0.0786633\ttotal: 43.7s\tremaining: 1m 16s\n",
      "364:\tlearn: 0.0786111\ttotal: 43.8s\tremaining: 1m 16s\n",
      "365:\tlearn: 0.0785626\ttotal: 44s\tremaining: 1m 16s\n",
      "366:\tlearn: 0.0784863\ttotal: 44.1s\tremaining: 1m 16s\n",
      "367:\tlearn: 0.0784054\ttotal: 44.2s\tremaining: 1m 15s\n",
      "368:\tlearn: 0.0783660\ttotal: 44.3s\tremaining: 1m 15s\n",
      "369:\tlearn: 0.0783174\ttotal: 44.4s\tremaining: 1m 15s\n",
      "370:\tlearn: 0.0782934\ttotal: 44.5s\tremaining: 1m 15s\n",
      "371:\tlearn: 0.0781576\ttotal: 44.7s\tremaining: 1m 15s\n",
      "372:\tlearn: 0.0780838\ttotal: 44.8s\tremaining: 1m 15s\n",
      "373:\tlearn: 0.0780606\ttotal: 44.9s\tremaining: 1m 15s\n",
      "374:\tlearn: 0.0779945\ttotal: 45s\tremaining: 1m 15s\n",
      "375:\tlearn: 0.0779501\ttotal: 45.1s\tremaining: 1m 14s\n",
      "376:\tlearn: 0.0779014\ttotal: 45.3s\tremaining: 1m 14s\n",
      "377:\tlearn: 0.0778728\ttotal: 45.4s\tremaining: 1m 14s\n",
      "378:\tlearn: 0.0778355\ttotal: 45.5s\tremaining: 1m 14s\n",
      "379:\tlearn: 0.0777783\ttotal: 45.7s\tremaining: 1m 14s\n",
      "380:\tlearn: 0.0777107\ttotal: 45.8s\tremaining: 1m 14s\n",
      "381:\tlearn: 0.0776390\ttotal: 45.9s\tremaining: 1m 14s\n",
      "382:\tlearn: 0.0775765\ttotal: 46s\tremaining: 1m 14s\n",
      "383:\tlearn: 0.0774356\ttotal: 46.2s\tremaining: 1m 14s\n",
      "384:\tlearn: 0.0772786\ttotal: 46.3s\tremaining: 1m 13s\n",
      "385:\tlearn: 0.0772238\ttotal: 46.4s\tremaining: 1m 13s\n",
      "386:\tlearn: 0.0771927\ttotal: 46.5s\tremaining: 1m 13s\n",
      "387:\tlearn: 0.0771563\ttotal: 46.7s\tremaining: 1m 13s\n",
      "388:\tlearn: 0.0771264\ttotal: 46.8s\tremaining: 1m 13s\n",
      "389:\tlearn: 0.0770736\ttotal: 46.9s\tremaining: 1m 13s\n",
      "390:\tlearn: 0.0770350\ttotal: 47s\tremaining: 1m 13s\n",
      "391:\tlearn: 0.0769596\ttotal: 47.1s\tremaining: 1m 13s\n",
      "392:\tlearn: 0.0768956\ttotal: 47.3s\tremaining: 1m 12s\n",
      "393:\tlearn: 0.0768306\ttotal: 47.4s\tremaining: 1m 12s\n",
      "394:\tlearn: 0.0767784\ttotal: 47.5s\tremaining: 1m 12s\n",
      "395:\tlearn: 0.0767569\ttotal: 47.6s\tremaining: 1m 12s\n",
      "396:\tlearn: 0.0767212\ttotal: 47.8s\tremaining: 1m 12s\n",
      "397:\tlearn: 0.0766593\ttotal: 47.9s\tremaining: 1m 12s\n",
      "398:\tlearn: 0.0766372\ttotal: 48s\tremaining: 1m 12s\n",
      "399:\tlearn: 0.0766103\ttotal: 48.1s\tremaining: 1m 12s\n",
      "400:\tlearn: 0.0764728\ttotal: 48.3s\tremaining: 1m 12s\n",
      "401:\tlearn: 0.0764302\ttotal: 48.4s\tremaining: 1m 11s\n",
      "402:\tlearn: 0.0763956\ttotal: 48.5s\tremaining: 1m 11s\n",
      "403:\tlearn: 0.0763590\ttotal: 48.6s\tremaining: 1m 11s\n",
      "404:\tlearn: 0.0763195\ttotal: 48.7s\tremaining: 1m 11s\n",
      "405:\tlearn: 0.0762651\ttotal: 48.8s\tremaining: 1m 11s\n",
      "406:\tlearn: 0.0762464\ttotal: 48.9s\tremaining: 1m 11s\n",
      "407:\tlearn: 0.0760894\ttotal: 49.1s\tremaining: 1m 11s\n",
      "408:\tlearn: 0.0760812\ttotal: 49.1s\tremaining: 1m 11s\n",
      "409:\tlearn: 0.0759933\ttotal: 49.3s\tremaining: 1m 10s\n",
      "410:\tlearn: 0.0759610\ttotal: 49.4s\tremaining: 1m 10s\n",
      "411:\tlearn: 0.0759335\ttotal: 49.5s\tremaining: 1m 10s\n",
      "412:\tlearn: 0.0759096\ttotal: 49.6s\tremaining: 1m 10s\n",
      "413:\tlearn: 0.0758328\ttotal: 49.7s\tremaining: 1m 10s\n",
      "414:\tlearn: 0.0757915\ttotal: 49.8s\tremaining: 1m 10s\n",
      "415:\tlearn: 0.0757270\ttotal: 50s\tremaining: 1m 10s\n",
      "416:\tlearn: 0.0756785\ttotal: 50.1s\tremaining: 1m 10s\n",
      "417:\tlearn: 0.0756476\ttotal: 50.2s\tremaining: 1m 9s\n",
      "418:\tlearn: 0.0755810\ttotal: 50.3s\tremaining: 1m 9s\n",
      "419:\tlearn: 0.0755436\ttotal: 50.4s\tremaining: 1m 9s\n",
      "420:\tlearn: 0.0754898\ttotal: 50.6s\tremaining: 1m 9s\n",
      "421:\tlearn: 0.0754515\ttotal: 50.7s\tremaining: 1m 9s\n",
      "422:\tlearn: 0.0754156\ttotal: 50.8s\tremaining: 1m 9s\n",
      "423:\tlearn: 0.0753748\ttotal: 50.9s\tremaining: 1m 9s\n",
      "424:\tlearn: 0.0753200\ttotal: 51.1s\tremaining: 1m 9s\n",
      "425:\tlearn: 0.0752753\ttotal: 51.2s\tremaining: 1m 8s\n",
      "426:\tlearn: 0.0752169\ttotal: 51.3s\tremaining: 1m 8s\n",
      "427:\tlearn: 0.0751784\ttotal: 51.4s\tremaining: 1m 8s\n",
      "428:\tlearn: 0.0751271\ttotal: 51.5s\tremaining: 1m 8s\n",
      "429:\tlearn: 0.0750566\ttotal: 51.7s\tremaining: 1m 8s\n",
      "430:\tlearn: 0.0750007\ttotal: 51.8s\tremaining: 1m 8s\n",
      "431:\tlearn: 0.0749667\ttotal: 51.9s\tremaining: 1m 8s\n",
      "432:\tlearn: 0.0749179\ttotal: 52s\tremaining: 1m 8s\n",
      "433:\tlearn: 0.0748753\ttotal: 52.1s\tremaining: 1m 7s\n",
      "434:\tlearn: 0.0748537\ttotal: 52.2s\tremaining: 1m 7s\n",
      "435:\tlearn: 0.0748249\ttotal: 52.3s\tremaining: 1m 7s\n",
      "436:\tlearn: 0.0747872\ttotal: 52.5s\tremaining: 1m 7s\n",
      "437:\tlearn: 0.0747586\ttotal: 52.6s\tremaining: 1m 7s\n",
      "438:\tlearn: 0.0747204\ttotal: 52.7s\tremaining: 1m 7s\n",
      "439:\tlearn: 0.0746920\ttotal: 52.8s\tremaining: 1m 7s\n",
      "440:\tlearn: 0.0746575\ttotal: 53s\tremaining: 1m 7s\n",
      "441:\tlearn: 0.0746175\ttotal: 53.1s\tremaining: 1m 7s\n",
      "442:\tlearn: 0.0745777\ttotal: 53.2s\tremaining: 1m 6s\n",
      "443:\tlearn: 0.0745268\ttotal: 53.3s\tremaining: 1m 6s\n",
      "444:\tlearn: 0.0744882\ttotal: 53.5s\tremaining: 1m 6s\n",
      "445:\tlearn: 0.0744594\ttotal: 53.6s\tremaining: 1m 6s\n",
      "446:\tlearn: 0.0744397\ttotal: 53.7s\tremaining: 1m 6s\n",
      "447:\tlearn: 0.0743915\ttotal: 53.8s\tremaining: 1m 6s\n",
      "448:\tlearn: 0.0743676\ttotal: 53.9s\tremaining: 1m 6s\n",
      "449:\tlearn: 0.0743463\ttotal: 54s\tremaining: 1m 6s\n",
      "450:\tlearn: 0.0742767\ttotal: 54.1s\tremaining: 1m 5s\n",
      "451:\tlearn: 0.0742367\ttotal: 54.2s\tremaining: 1m 5s\n",
      "452:\tlearn: 0.0741424\ttotal: 54.4s\tremaining: 1m 5s\n",
      "453:\tlearn: 0.0741120\ttotal: 54.5s\tremaining: 1m 5s\n",
      "454:\tlearn: 0.0740828\ttotal: 54.6s\tremaining: 1m 5s\n",
      "455:\tlearn: 0.0740603\ttotal: 54.7s\tremaining: 1m 5s\n",
      "456:\tlearn: 0.0740310\ttotal: 54.8s\tremaining: 1m 5s\n",
      "457:\tlearn: 0.0739734\ttotal: 54.9s\tremaining: 1m 5s\n",
      "458:\tlearn: 0.0739526\ttotal: 55.1s\tremaining: 1m 4s\n",
      "459:\tlearn: 0.0739076\ttotal: 55.2s\tremaining: 1m 4s\n",
      "460:\tlearn: 0.0738655\ttotal: 55.3s\tremaining: 1m 4s\n",
      "461:\tlearn: 0.0738357\ttotal: 55.4s\tremaining: 1m 4s\n",
      "462:\tlearn: 0.0738211\ttotal: 55.5s\tremaining: 1m 4s\n",
      "463:\tlearn: 0.0737942\ttotal: 55.7s\tremaining: 1m 4s\n",
      "464:\tlearn: 0.0737589\ttotal: 55.8s\tremaining: 1m 4s\n",
      "465:\tlearn: 0.0737092\ttotal: 55.9s\tremaining: 1m 4s\n",
      "466:\tlearn: 0.0736764\ttotal: 56s\tremaining: 1m 3s\n",
      "467:\tlearn: 0.0736330\ttotal: 56.2s\tremaining: 1m 3s\n",
      "468:\tlearn: 0.0735576\ttotal: 56.3s\tremaining: 1m 3s\n",
      "469:\tlearn: 0.0735380\ttotal: 56.4s\tremaining: 1m 3s\n",
      "470:\tlearn: 0.0734953\ttotal: 56.6s\tremaining: 1m 3s\n",
      "471:\tlearn: 0.0734619\ttotal: 56.7s\tremaining: 1m 3s\n",
      "472:\tlearn: 0.0734018\ttotal: 56.8s\tremaining: 1m 3s\n",
      "473:\tlearn: 0.0733484\ttotal: 57s\tremaining: 1m 3s\n",
      "474:\tlearn: 0.0732923\ttotal: 57.1s\tremaining: 1m 3s\n",
      "475:\tlearn: 0.0732541\ttotal: 57.2s\tremaining: 1m 3s\n",
      "476:\tlearn: 0.0732377\ttotal: 57.3s\tremaining: 1m 2s\n",
      "477:\tlearn: 0.0732192\ttotal: 57.5s\tremaining: 1m 2s\n",
      "478:\tlearn: 0.0731633\ttotal: 57.6s\tremaining: 1m 2s\n",
      "479:\tlearn: 0.0731471\ttotal: 57.7s\tremaining: 1m 2s\n",
      "480:\tlearn: 0.0731034\ttotal: 57.8s\tremaining: 1m 2s\n",
      "481:\tlearn: 0.0730631\ttotal: 58s\tremaining: 1m 2s\n",
      "482:\tlearn: 0.0730281\ttotal: 58.1s\tremaining: 1m 2s\n",
      "483:\tlearn: 0.0729794\ttotal: 58.2s\tremaining: 1m 2s\n",
      "484:\tlearn: 0.0729118\ttotal: 58.4s\tremaining: 1m 2s\n",
      "485:\tlearn: 0.0728662\ttotal: 58.5s\tremaining: 1m 1s\n",
      "486:\tlearn: 0.0728042\ttotal: 58.6s\tremaining: 1m 1s\n",
      "487:\tlearn: 0.0727418\ttotal: 58.8s\tremaining: 1m 1s\n",
      "488:\tlearn: 0.0726997\ttotal: 58.9s\tremaining: 1m 1s\n",
      "489:\tlearn: 0.0726590\ttotal: 59s\tremaining: 1m 1s\n",
      "490:\tlearn: 0.0725927\ttotal: 59.1s\tremaining: 1m 1s\n",
      "491:\tlearn: 0.0725733\ttotal: 59.2s\tremaining: 1m 1s\n",
      "492:\tlearn: 0.0725356\ttotal: 59.3s\tremaining: 1m 1s\n",
      "493:\tlearn: 0.0724871\ttotal: 59.5s\tremaining: 1m\n",
      "494:\tlearn: 0.0724574\ttotal: 59.6s\tremaining: 1m\n",
      "495:\tlearn: 0.0724185\ttotal: 59.7s\tremaining: 1m\n",
      "496:\tlearn: 0.0723241\ttotal: 59.9s\tremaining: 1m\n",
      "497:\tlearn: 0.0722587\ttotal: 60s\tremaining: 1m\n",
      "498:\tlearn: 0.0722172\ttotal: 1m\tremaining: 1m\n",
      "499:\tlearn: 0.0721792\ttotal: 1m\tremaining: 1m\n",
      "500:\tlearn: 0.0721583\ttotal: 1m\tremaining: 1m\n",
      "501:\tlearn: 0.0721148\ttotal: 1m\tremaining: 60s\n",
      "502:\tlearn: 0.0720738\ttotal: 1m\tremaining: 59.9s\n",
      "503:\tlearn: 0.0720419\ttotal: 1m\tremaining: 59.7s\n",
      "504:\tlearn: 0.0720124\ttotal: 1m\tremaining: 59.6s\n",
      "505:\tlearn: 0.0719880\ttotal: 1m\tremaining: 59.5s\n",
      "506:\tlearn: 0.0719614\ttotal: 1m 1s\tremaining: 59.4s\n",
      "507:\tlearn: 0.0719281\ttotal: 1m 1s\tremaining: 59.2s\n",
      "508:\tlearn: 0.0718627\ttotal: 1m 1s\tremaining: 59.1s\n",
      "509:\tlearn: 0.0718363\ttotal: 1m 1s\tremaining: 59s\n",
      "510:\tlearn: 0.0717976\ttotal: 1m 1s\tremaining: 58.8s\n",
      "511:\tlearn: 0.0717425\ttotal: 1m 1s\tremaining: 58.7s\n",
      "512:\tlearn: 0.0717108\ttotal: 1m 1s\tremaining: 58.6s\n",
      "513:\tlearn: 0.0716916\ttotal: 1m 1s\tremaining: 58.5s\n",
      "514:\tlearn: 0.0716713\ttotal: 1m 1s\tremaining: 58.3s\n",
      "515:\tlearn: 0.0716478\ttotal: 1m 2s\tremaining: 58.2s\n",
      "516:\tlearn: 0.0715856\ttotal: 1m 2s\tremaining: 58.1s\n",
      "517:\tlearn: 0.0715587\ttotal: 1m 2s\tremaining: 58s\n",
      "518:\tlearn: 0.0715105\ttotal: 1m 2s\tremaining: 57.8s\n",
      "519:\tlearn: 0.0714594\ttotal: 1m 2s\tremaining: 57.7s\n",
      "520:\tlearn: 0.0714357\ttotal: 1m 2s\tremaining: 57.6s\n",
      "521:\tlearn: 0.0714092\ttotal: 1m 2s\tremaining: 57.5s\n",
      "522:\tlearn: 0.0713823\ttotal: 1m 2s\tremaining: 57.3s\n",
      "523:\tlearn: 0.0713359\ttotal: 1m 2s\tremaining: 57.2s\n",
      "524:\tlearn: 0.0713112\ttotal: 1m 3s\tremaining: 57.1s\n",
      "525:\tlearn: 0.0712998\ttotal: 1m 3s\tremaining: 57s\n",
      "526:\tlearn: 0.0712297\ttotal: 1m 3s\tremaining: 56.9s\n",
      "527:\tlearn: 0.0711949\ttotal: 1m 3s\tremaining: 56.8s\n",
      "528:\tlearn: 0.0711386\ttotal: 1m 3s\tremaining: 56.6s\n",
      "529:\tlearn: 0.0711071\ttotal: 1m 3s\tremaining: 56.5s\n",
      "530:\tlearn: 0.0710815\ttotal: 1m 3s\tremaining: 56.4s\n",
      "531:\tlearn: 0.0710533\ttotal: 1m 3s\tremaining: 56.3s\n",
      "532:\tlearn: 0.0710175\ttotal: 1m 4s\tremaining: 56.1s\n",
      "533:\tlearn: 0.0710060\ttotal: 1m 4s\tremaining: 56s\n",
      "534:\tlearn: 0.0709664\ttotal: 1m 4s\tremaining: 55.9s\n",
      "535:\tlearn: 0.0709407\ttotal: 1m 4s\tremaining: 55.8s\n",
      "536:\tlearn: 0.0709219\ttotal: 1m 4s\tremaining: 55.6s\n",
      "537:\tlearn: 0.0708902\ttotal: 1m 4s\tremaining: 55.5s\n",
      "538:\tlearn: 0.0708586\ttotal: 1m 4s\tremaining: 55.4s\n",
      "539:\tlearn: 0.0708316\ttotal: 1m 4s\tremaining: 55.3s\n",
      "540:\tlearn: 0.0708055\ttotal: 1m 5s\tremaining: 55.2s\n",
      "541:\tlearn: 0.0707858\ttotal: 1m 5s\tremaining: 55s\n",
      "542:\tlearn: 0.0707565\ttotal: 1m 5s\tremaining: 54.9s\n",
      "543:\tlearn: 0.0707230\ttotal: 1m 5s\tremaining: 54.8s\n",
      "544:\tlearn: 0.0706840\ttotal: 1m 5s\tremaining: 54.7s\n",
      "545:\tlearn: 0.0706640\ttotal: 1m 5s\tremaining: 54.6s\n",
      "546:\tlearn: 0.0706562\ttotal: 1m 5s\tremaining: 54.4s\n",
      "547:\tlearn: 0.0706301\ttotal: 1m 5s\tremaining: 54.3s\n",
      "548:\tlearn: 0.0705551\ttotal: 1m 5s\tremaining: 54.2s\n",
      "549:\tlearn: 0.0705296\ttotal: 1m 6s\tremaining: 54.1s\n",
      "550:\tlearn: 0.0704982\ttotal: 1m 6s\tremaining: 53.9s\n",
      "551:\tlearn: 0.0704810\ttotal: 1m 6s\tremaining: 53.8s\n",
      "552:\tlearn: 0.0704441\ttotal: 1m 6s\tremaining: 53.7s\n",
      "553:\tlearn: 0.0704267\ttotal: 1m 6s\tremaining: 53.6s\n",
      "554:\tlearn: 0.0703862\ttotal: 1m 6s\tremaining: 53.5s\n",
      "555:\tlearn: 0.0703517\ttotal: 1m 6s\tremaining: 53.4s\n",
      "556:\tlearn: 0.0703121\ttotal: 1m 6s\tremaining: 53.3s\n",
      "557:\tlearn: 0.0702531\ttotal: 1m 7s\tremaining: 53.2s\n",
      "558:\tlearn: 0.0701793\ttotal: 1m 7s\tremaining: 53s\n",
      "559:\tlearn: 0.0701610\ttotal: 1m 7s\tremaining: 52.9s\n",
      "560:\tlearn: 0.0701165\ttotal: 1m 7s\tremaining: 52.8s\n",
      "561:\tlearn: 0.0701090\ttotal: 1m 7s\tremaining: 52.7s\n",
      "562:\tlearn: 0.0700898\ttotal: 1m 7s\tremaining: 52.6s\n",
      "563:\tlearn: 0.0700536\ttotal: 1m 7s\tremaining: 52.4s\n",
      "564:\tlearn: 0.0700344\ttotal: 1m 7s\tremaining: 52.3s\n",
      "565:\tlearn: 0.0700035\ttotal: 1m 8s\tremaining: 52.2s\n",
      "566:\tlearn: 0.0699839\ttotal: 1m 8s\tremaining: 52.1s\n",
      "567:\tlearn: 0.0699653\ttotal: 1m 8s\tremaining: 51.9s\n",
      "568:\tlearn: 0.0699185\ttotal: 1m 8s\tremaining: 51.8s\n",
      "569:\tlearn: 0.0698804\ttotal: 1m 8s\tremaining: 51.7s\n",
      "570:\tlearn: 0.0698257\ttotal: 1m 8s\tremaining: 51.6s\n",
      "571:\tlearn: 0.0697587\ttotal: 1m 8s\tremaining: 51.5s\n",
      "572:\tlearn: 0.0697176\ttotal: 1m 8s\tremaining: 51.4s\n",
      "573:\tlearn: 0.0696899\ttotal: 1m 9s\tremaining: 51.2s\n",
      "574:\tlearn: 0.0696513\ttotal: 1m 9s\tremaining: 51.1s\n",
      "575:\tlearn: 0.0696138\ttotal: 1m 9s\tremaining: 51s\n",
      "576:\tlearn: 0.0696013\ttotal: 1m 9s\tremaining: 50.9s\n",
      "577:\tlearn: 0.0695849\ttotal: 1m 9s\tremaining: 50.8s\n",
      "578:\tlearn: 0.0695664\ttotal: 1m 9s\tremaining: 50.6s\n",
      "579:\tlearn: 0.0695081\ttotal: 1m 9s\tremaining: 50.5s\n",
      "580:\tlearn: 0.0694602\ttotal: 1m 9s\tremaining: 50.4s\n",
      "581:\tlearn: 0.0694403\ttotal: 1m 9s\tremaining: 50.3s\n",
      "582:\tlearn: 0.0694247\ttotal: 1m 10s\tremaining: 50.1s\n",
      "583:\tlearn: 0.0693772\ttotal: 1m 10s\tremaining: 50s\n",
      "584:\tlearn: 0.0693469\ttotal: 1m 10s\tremaining: 49.9s\n",
      "585:\tlearn: 0.0693181\ttotal: 1m 10s\tremaining: 49.8s\n",
      "586:\tlearn: 0.0692740\ttotal: 1m 10s\tremaining: 49.7s\n",
      "587:\tlearn: 0.0692303\ttotal: 1m 10s\tremaining: 49.5s\n",
      "588:\tlearn: 0.0691559\ttotal: 1m 10s\tremaining: 49.4s\n",
      "589:\tlearn: 0.0691368\ttotal: 1m 10s\tremaining: 49.3s\n",
      "590:\tlearn: 0.0690978\ttotal: 1m 11s\tremaining: 49.2s\n",
      "591:\tlearn: 0.0690710\ttotal: 1m 11s\tremaining: 49.1s\n",
      "592:\tlearn: 0.0690366\ttotal: 1m 11s\tremaining: 49s\n",
      "593:\tlearn: 0.0690023\ttotal: 1m 11s\tremaining: 48.8s\n",
      "594:\tlearn: 0.0689753\ttotal: 1m 11s\tremaining: 48.7s\n",
      "595:\tlearn: 0.0689348\ttotal: 1m 11s\tremaining: 48.6s\n",
      "596:\tlearn: 0.0688924\ttotal: 1m 11s\tremaining: 48.5s\n",
      "597:\tlearn: 0.0688708\ttotal: 1m 11s\tremaining: 48.4s\n",
      "598:\tlearn: 0.0688615\ttotal: 1m 12s\tremaining: 48.2s\n",
      "599:\tlearn: 0.0688478\ttotal: 1m 12s\tremaining: 48.1s\n",
      "600:\tlearn: 0.0688308\ttotal: 1m 12s\tremaining: 48s\n",
      "601:\tlearn: 0.0688000\ttotal: 1m 12s\tremaining: 47.9s\n",
      "602:\tlearn: 0.0687748\ttotal: 1m 12s\tremaining: 47.7s\n",
      "603:\tlearn: 0.0687625\ttotal: 1m 12s\tremaining: 47.6s\n",
      "604:\tlearn: 0.0687513\ttotal: 1m 12s\tremaining: 47.5s\n",
      "605:\tlearn: 0.0687207\ttotal: 1m 12s\tremaining: 47.4s\n",
      "606:\tlearn: 0.0686777\ttotal: 1m 12s\tremaining: 47.3s\n",
      "607:\tlearn: 0.0686318\ttotal: 1m 13s\tremaining: 47.1s\n",
      "608:\tlearn: 0.0686097\ttotal: 1m 13s\tremaining: 47s\n",
      "609:\tlearn: 0.0685762\ttotal: 1m 13s\tremaining: 46.9s\n",
      "610:\tlearn: 0.0685377\ttotal: 1m 13s\tremaining: 46.8s\n",
      "611:\tlearn: 0.0685092\ttotal: 1m 13s\tremaining: 46.7s\n",
      "612:\tlearn: 0.0684938\ttotal: 1m 13s\tremaining: 46.5s\n",
      "613:\tlearn: 0.0684556\ttotal: 1m 13s\tremaining: 46.4s\n",
      "614:\tlearn: 0.0684235\ttotal: 1m 13s\tremaining: 46.3s\n",
      "615:\tlearn: 0.0683979\ttotal: 1m 14s\tremaining: 46.2s\n",
      "616:\tlearn: 0.0683761\ttotal: 1m 14s\tremaining: 46.1s\n",
      "617:\tlearn: 0.0683162\ttotal: 1m 14s\tremaining: 46s\n",
      "618:\tlearn: 0.0682916\ttotal: 1m 14s\tremaining: 45.8s\n",
      "619:\tlearn: 0.0682737\ttotal: 1m 14s\tremaining: 45.7s\n",
      "620:\tlearn: 0.0682379\ttotal: 1m 14s\tremaining: 45.6s\n",
      "621:\tlearn: 0.0682249\ttotal: 1m 14s\tremaining: 45.5s\n",
      "622:\tlearn: 0.0681827\ttotal: 1m 14s\tremaining: 45.4s\n",
      "623:\tlearn: 0.0681635\ttotal: 1m 15s\tremaining: 45.2s\n",
      "624:\tlearn: 0.0681347\ttotal: 1m 15s\tremaining: 45.1s\n",
      "625:\tlearn: 0.0681102\ttotal: 1m 15s\tremaining: 45s\n",
      "626:\tlearn: 0.0680813\ttotal: 1m 15s\tremaining: 44.9s\n",
      "627:\tlearn: 0.0680613\ttotal: 1m 15s\tremaining: 44.8s\n",
      "628:\tlearn: 0.0680110\ttotal: 1m 15s\tremaining: 44.6s\n",
      "629:\tlearn: 0.0679823\ttotal: 1m 15s\tremaining: 44.5s\n",
      "630:\tlearn: 0.0679700\ttotal: 1m 15s\tremaining: 44.4s\n",
      "631:\tlearn: 0.0679392\ttotal: 1m 16s\tremaining: 44.3s\n",
      "632:\tlearn: 0.0679040\ttotal: 1m 16s\tremaining: 44.2s\n",
      "633:\tlearn: 0.0678217\ttotal: 1m 16s\tremaining: 44.1s\n",
      "634:\tlearn: 0.0677824\ttotal: 1m 16s\tremaining: 43.9s\n",
      "635:\tlearn: 0.0677541\ttotal: 1m 16s\tremaining: 43.8s\n",
      "636:\tlearn: 0.0677346\ttotal: 1m 16s\tremaining: 43.7s\n",
      "637:\tlearn: 0.0677346\ttotal: 1m 16s\tremaining: 43.6s\n",
      "638:\tlearn: 0.0677017\ttotal: 1m 16s\tremaining: 43.5s\n",
      "639:\tlearn: 0.0676988\ttotal: 1m 17s\tremaining: 43.3s\n",
      "640:\tlearn: 0.0676758\ttotal: 1m 17s\tremaining: 43.2s\n",
      "641:\tlearn: 0.0676018\ttotal: 1m 17s\tremaining: 43.1s\n",
      "642:\tlearn: 0.0675776\ttotal: 1m 17s\tremaining: 43s\n",
      "643:\tlearn: 0.0675670\ttotal: 1m 17s\tremaining: 42.8s\n",
      "644:\tlearn: 0.0675539\ttotal: 1m 17s\tremaining: 42.7s\n",
      "645:\tlearn: 0.0675397\ttotal: 1m 17s\tremaining: 42.6s\n",
      "646:\tlearn: 0.0675106\ttotal: 1m 17s\tremaining: 42.5s\n",
      "647:\tlearn: 0.0675036\ttotal: 1m 17s\tremaining: 42.3s\n",
      "648:\tlearn: 0.0674593\ttotal: 1m 18s\tremaining: 42.2s\n",
      "649:\tlearn: 0.0674248\ttotal: 1m 18s\tremaining: 42.1s\n",
      "650:\tlearn: 0.0673998\ttotal: 1m 18s\tremaining: 42s\n",
      "651:\tlearn: 0.0673713\ttotal: 1m 18s\tremaining: 41.9s\n",
      "652:\tlearn: 0.0673523\ttotal: 1m 18s\tremaining: 41.7s\n",
      "653:\tlearn: 0.0673334\ttotal: 1m 18s\tremaining: 41.6s\n",
      "654:\tlearn: 0.0673165\ttotal: 1m 18s\tremaining: 41.5s\n",
      "655:\tlearn: 0.0672667\ttotal: 1m 18s\tremaining: 41.4s\n",
      "656:\tlearn: 0.0672566\ttotal: 1m 19s\tremaining: 41.3s\n",
      "657:\tlearn: 0.0671763\ttotal: 1m 19s\tremaining: 41.2s\n",
      "658:\tlearn: 0.0671628\ttotal: 1m 19s\tremaining: 41s\n",
      "659:\tlearn: 0.0671415\ttotal: 1m 19s\tremaining: 40.9s\n",
      "660:\tlearn: 0.0671235\ttotal: 1m 19s\tremaining: 40.8s\n",
      "661:\tlearn: 0.0670852\ttotal: 1m 19s\tremaining: 40.7s\n",
      "662:\tlearn: 0.0670637\ttotal: 1m 19s\tremaining: 40.6s\n",
      "663:\tlearn: 0.0670363\ttotal: 1m 19s\tremaining: 40.4s\n",
      "664:\tlearn: 0.0670115\ttotal: 1m 20s\tremaining: 40.3s\n",
      "665:\tlearn: 0.0669625\ttotal: 1m 20s\tremaining: 40.2s\n",
      "666:\tlearn: 0.0669561\ttotal: 1m 20s\tremaining: 40.1s\n",
      "667:\tlearn: 0.0669223\ttotal: 1m 20s\tremaining: 40s\n",
      "668:\tlearn: 0.0668931\ttotal: 1m 20s\tremaining: 39.9s\n",
      "669:\tlearn: 0.0668783\ttotal: 1m 20s\tremaining: 39.7s\n",
      "670:\tlearn: 0.0668657\ttotal: 1m 20s\tremaining: 39.6s\n",
      "671:\tlearn: 0.0668430\ttotal: 1m 20s\tremaining: 39.5s\n",
      "672:\tlearn: 0.0668247\ttotal: 1m 21s\tremaining: 39.4s\n",
      "673:\tlearn: 0.0667995\ttotal: 1m 21s\tremaining: 39.3s\n",
      "674:\tlearn: 0.0667745\ttotal: 1m 21s\tremaining: 39.2s\n",
      "675:\tlearn: 0.0667560\ttotal: 1m 21s\tremaining: 39s\n",
      "676:\tlearn: 0.0667325\ttotal: 1m 21s\tremaining: 38.9s\n",
      "677:\tlearn: 0.0666790\ttotal: 1m 21s\tremaining: 38.8s\n",
      "678:\tlearn: 0.0666638\ttotal: 1m 21s\tremaining: 38.7s\n",
      "679:\tlearn: 0.0666495\ttotal: 1m 21s\tremaining: 38.6s\n",
      "680:\tlearn: 0.0666128\ttotal: 1m 22s\tremaining: 38.4s\n",
      "681:\tlearn: 0.0665844\ttotal: 1m 22s\tremaining: 38.3s\n",
      "682:\tlearn: 0.0665651\ttotal: 1m 22s\tremaining: 38.2s\n",
      "683:\tlearn: 0.0665202\ttotal: 1m 22s\tremaining: 38.1s\n",
      "684:\tlearn: 0.0665032\ttotal: 1m 22s\tremaining: 38s\n",
      "685:\tlearn: 0.0664816\ttotal: 1m 22s\tremaining: 37.8s\n",
      "686:\tlearn: 0.0664657\ttotal: 1m 22s\tremaining: 37.7s\n",
      "687:\tlearn: 0.0664469\ttotal: 1m 22s\tremaining: 37.6s\n",
      "688:\tlearn: 0.0664151\ttotal: 1m 23s\tremaining: 37.5s\n",
      "689:\tlearn: 0.0664042\ttotal: 1m 23s\tremaining: 37.4s\n",
      "690:\tlearn: 0.0663738\ttotal: 1m 23s\tremaining: 37.2s\n",
      "691:\tlearn: 0.0663484\ttotal: 1m 23s\tremaining: 37.1s\n",
      "692:\tlearn: 0.0663245\ttotal: 1m 23s\tremaining: 37s\n",
      "693:\tlearn: 0.0662806\ttotal: 1m 23s\tremaining: 36.9s\n",
      "694:\tlearn: 0.0662532\ttotal: 1m 23s\tremaining: 36.8s\n",
      "695:\tlearn: 0.0662442\ttotal: 1m 23s\tremaining: 36.6s\n",
      "696:\tlearn: 0.0662295\ttotal: 1m 23s\tremaining: 36.5s\n",
      "697:\tlearn: 0.0662149\ttotal: 1m 24s\tremaining: 36.4s\n",
      "698:\tlearn: 0.0661661\ttotal: 1m 24s\tremaining: 36.3s\n",
      "699:\tlearn: 0.0661538\ttotal: 1m 24s\tremaining: 36.1s\n",
      "700:\tlearn: 0.0661271\ttotal: 1m 24s\tremaining: 36s\n",
      "701:\tlearn: 0.0661061\ttotal: 1m 24s\tremaining: 35.9s\n",
      "702:\tlearn: 0.0660892\ttotal: 1m 24s\tremaining: 35.8s\n",
      "703:\tlearn: 0.0660649\ttotal: 1m 24s\tremaining: 35.7s\n",
      "704:\tlearn: 0.0660396\ttotal: 1m 24s\tremaining: 35.5s\n",
      "705:\tlearn: 0.0660158\ttotal: 1m 25s\tremaining: 35.4s\n",
      "706:\tlearn: 0.0659919\ttotal: 1m 25s\tremaining: 35.3s\n",
      "707:\tlearn: 0.0659453\ttotal: 1m 25s\tremaining: 35.2s\n",
      "708:\tlearn: 0.0659269\ttotal: 1m 25s\tremaining: 35s\n",
      "709:\tlearn: 0.0658107\ttotal: 1m 25s\tremaining: 34.9s\n",
      "710:\tlearn: 0.0657859\ttotal: 1m 25s\tremaining: 34.8s\n",
      "711:\tlearn: 0.0657602\ttotal: 1m 25s\tremaining: 34.7s\n",
      "712:\tlearn: 0.0657402\ttotal: 1m 25s\tremaining: 34.6s\n",
      "713:\tlearn: 0.0657238\ttotal: 1m 25s\tremaining: 34.4s\n",
      "714:\tlearn: 0.0656984\ttotal: 1m 26s\tremaining: 34.3s\n",
      "715:\tlearn: 0.0656751\ttotal: 1m 26s\tremaining: 34.2s\n",
      "716:\tlearn: 0.0656558\ttotal: 1m 26s\tremaining: 34.1s\n",
      "717:\tlearn: 0.0656385\ttotal: 1m 26s\tremaining: 34s\n",
      "718:\tlearn: 0.0656104\ttotal: 1m 26s\tremaining: 33.8s\n",
      "719:\tlearn: 0.0655834\ttotal: 1m 26s\tremaining: 33.7s\n",
      "720:\tlearn: 0.0655563\ttotal: 1m 26s\tremaining: 33.6s\n",
      "721:\tlearn: 0.0655253\ttotal: 1m 26s\tremaining: 33.5s\n",
      "722:\tlearn: 0.0655083\ttotal: 1m 27s\tremaining: 33.4s\n",
      "723:\tlearn: 0.0654782\ttotal: 1m 27s\tremaining: 33.2s\n",
      "724:\tlearn: 0.0654655\ttotal: 1m 27s\tremaining: 33.1s\n",
      "725:\tlearn: 0.0654464\ttotal: 1m 27s\tremaining: 33s\n",
      "726:\tlearn: 0.0654148\ttotal: 1m 27s\tremaining: 32.9s\n",
      "727:\tlearn: 0.0653929\ttotal: 1m 27s\tremaining: 32.8s\n",
      "728:\tlearn: 0.0653758\ttotal: 1m 27s\tremaining: 32.6s\n",
      "729:\tlearn: 0.0653535\ttotal: 1m 27s\tremaining: 32.5s\n",
      "730:\tlearn: 0.0653253\ttotal: 1m 28s\tremaining: 32.4s\n",
      "731:\tlearn: 0.0653123\ttotal: 1m 28s\tremaining: 32.3s\n",
      "732:\tlearn: 0.0652767\ttotal: 1m 28s\tremaining: 32.1s\n",
      "733:\tlearn: 0.0652448\ttotal: 1m 28s\tremaining: 32s\n",
      "734:\tlearn: 0.0652331\ttotal: 1m 28s\tremaining: 31.9s\n",
      "735:\tlearn: 0.0651794\ttotal: 1m 28s\tremaining: 31.8s\n",
      "736:\tlearn: 0.0651512\ttotal: 1m 28s\tremaining: 31.7s\n",
      "737:\tlearn: 0.0651252\ttotal: 1m 28s\tremaining: 31.5s\n",
      "738:\tlearn: 0.0651039\ttotal: 1m 28s\tremaining: 31.4s\n",
      "739:\tlearn: 0.0651038\ttotal: 1m 29s\tremaining: 31.3s\n",
      "740:\tlearn: 0.0650790\ttotal: 1m 29s\tremaining: 31.2s\n",
      "741:\tlearn: 0.0650556\ttotal: 1m 29s\tremaining: 31.1s\n",
      "742:\tlearn: 0.0650399\ttotal: 1m 29s\tremaining: 30.9s\n",
      "743:\tlearn: 0.0650206\ttotal: 1m 29s\tremaining: 30.8s\n",
      "744:\tlearn: 0.0650142\ttotal: 1m 29s\tremaining: 30.7s\n",
      "745:\tlearn: 0.0650097\ttotal: 1m 29s\tremaining: 30.6s\n",
      "746:\tlearn: 0.0649948\ttotal: 1m 29s\tremaining: 30.4s\n",
      "747:\tlearn: 0.0649822\ttotal: 1m 29s\tremaining: 30.3s\n",
      "748:\tlearn: 0.0649532\ttotal: 1m 30s\tremaining: 30.2s\n",
      "749:\tlearn: 0.0649363\ttotal: 1m 30s\tremaining: 30.1s\n",
      "750:\tlearn: 0.0649012\ttotal: 1m 30s\tremaining: 30s\n",
      "751:\tlearn: 0.0648644\ttotal: 1m 30s\tremaining: 29.8s\n",
      "752:\tlearn: 0.0648643\ttotal: 1m 30s\tremaining: 29.7s\n",
      "753:\tlearn: 0.0648476\ttotal: 1m 30s\tremaining: 29.6s\n",
      "754:\tlearn: 0.0648216\ttotal: 1m 30s\tremaining: 29.5s\n",
      "755:\tlearn: 0.0648027\ttotal: 1m 30s\tremaining: 29.4s\n",
      "756:\tlearn: 0.0647584\ttotal: 1m 31s\tremaining: 29.2s\n",
      "757:\tlearn: 0.0647318\ttotal: 1m 31s\tremaining: 29.1s\n",
      "758:\tlearn: 0.0647222\ttotal: 1m 31s\tremaining: 29s\n",
      "759:\tlearn: 0.0647087\ttotal: 1m 31s\tremaining: 28.9s\n",
      "760:\tlearn: 0.0647072\ttotal: 1m 31s\tremaining: 28.7s\n",
      "761:\tlearn: 0.0646735\ttotal: 1m 31s\tremaining: 28.6s\n",
      "762:\tlearn: 0.0646584\ttotal: 1m 31s\tremaining: 28.5s\n",
      "763:\tlearn: 0.0646373\ttotal: 1m 31s\tremaining: 28.4s\n",
      "764:\tlearn: 0.0646091\ttotal: 1m 32s\tremaining: 28.3s\n",
      "765:\tlearn: 0.0645596\ttotal: 1m 32s\tremaining: 28.1s\n",
      "766:\tlearn: 0.0645259\ttotal: 1m 32s\tremaining: 28s\n",
      "767:\tlearn: 0.0645143\ttotal: 1m 32s\tremaining: 27.9s\n",
      "768:\tlearn: 0.0645014\ttotal: 1m 32s\tremaining: 27.8s\n",
      "769:\tlearn: 0.0644766\ttotal: 1m 32s\tremaining: 27.7s\n",
      "770:\tlearn: 0.0644661\ttotal: 1m 32s\tremaining: 27.5s\n",
      "771:\tlearn: 0.0644661\ttotal: 1m 32s\tremaining: 27.4s\n",
      "772:\tlearn: 0.0644510\ttotal: 1m 32s\tremaining: 27.3s\n",
      "773:\tlearn: 0.0644251\ttotal: 1m 33s\tremaining: 27.2s\n",
      "774:\tlearn: 0.0644251\ttotal: 1m 33s\tremaining: 27s\n",
      "775:\tlearn: 0.0644139\ttotal: 1m 33s\tremaining: 26.9s\n",
      "776:\tlearn: 0.0643855\ttotal: 1m 33s\tremaining: 26.8s\n",
      "777:\tlearn: 0.0643803\ttotal: 1m 33s\tremaining: 26.7s\n",
      "778:\tlearn: 0.0643612\ttotal: 1m 33s\tremaining: 26.6s\n",
      "779:\tlearn: 0.0643289\ttotal: 1m 33s\tremaining: 26.4s\n",
      "780:\tlearn: 0.0643150\ttotal: 1m 33s\tremaining: 26.3s\n",
      "781:\tlearn: 0.0643064\ttotal: 1m 33s\tremaining: 26.2s\n",
      "782:\tlearn: 0.0642893\ttotal: 1m 34s\tremaining: 26.1s\n",
      "783:\tlearn: 0.0642872\ttotal: 1m 34s\tremaining: 25.9s\n",
      "784:\tlearn: 0.0642336\ttotal: 1m 34s\tremaining: 25.8s\n",
      "785:\tlearn: 0.0641989\ttotal: 1m 34s\tremaining: 25.7s\n",
      "786:\tlearn: 0.0641548\ttotal: 1m 34s\tremaining: 25.6s\n",
      "787:\tlearn: 0.0641226\ttotal: 1m 34s\tremaining: 25.5s\n",
      "788:\tlearn: 0.0640957\ttotal: 1m 34s\tremaining: 25.4s\n",
      "789:\tlearn: 0.0640451\ttotal: 1m 34s\tremaining: 25.2s\n",
      "790:\tlearn: 0.0640240\ttotal: 1m 35s\tremaining: 25.1s\n",
      "791:\tlearn: 0.0640080\ttotal: 1m 35s\tremaining: 25s\n",
      "792:\tlearn: 0.0639717\ttotal: 1m 35s\tremaining: 24.9s\n",
      "793:\tlearn: 0.0639546\ttotal: 1m 35s\tremaining: 24.8s\n",
      "794:\tlearn: 0.0639491\ttotal: 1m 35s\tremaining: 24.6s\n",
      "795:\tlearn: 0.0639352\ttotal: 1m 35s\tremaining: 24.5s\n",
      "796:\tlearn: 0.0639120\ttotal: 1m 35s\tremaining: 24.4s\n",
      "797:\tlearn: 0.0639030\ttotal: 1m 35s\tremaining: 24.3s\n",
      "798:\tlearn: 0.0638546\ttotal: 1m 36s\tremaining: 24.2s\n",
      "799:\tlearn: 0.0638325\ttotal: 1m 36s\tremaining: 24s\n",
      "800:\tlearn: 0.0638131\ttotal: 1m 36s\tremaining: 23.9s\n",
      "801:\tlearn: 0.0638051\ttotal: 1m 36s\tremaining: 23.8s\n",
      "802:\tlearn: 0.0637719\ttotal: 1m 36s\tremaining: 23.7s\n",
      "803:\tlearn: 0.0637514\ttotal: 1m 36s\tremaining: 23.6s\n",
      "804:\tlearn: 0.0637436\ttotal: 1m 36s\tremaining: 23.4s\n",
      "805:\tlearn: 0.0637266\ttotal: 1m 36s\tremaining: 23.3s\n",
      "806:\tlearn: 0.0636968\ttotal: 1m 36s\tremaining: 23.2s\n",
      "807:\tlearn: 0.0636795\ttotal: 1m 37s\tremaining: 23.1s\n",
      "808:\tlearn: 0.0636417\ttotal: 1m 37s\tremaining: 23s\n",
      "809:\tlearn: 0.0636188\ttotal: 1m 37s\tremaining: 22.8s\n",
      "810:\tlearn: 0.0636109\ttotal: 1m 37s\tremaining: 22.7s\n",
      "811:\tlearn: 0.0636109\ttotal: 1m 37s\tremaining: 22.6s\n",
      "812:\tlearn: 0.0635966\ttotal: 1m 37s\tremaining: 22.5s\n",
      "813:\tlearn: 0.0635758\ttotal: 1m 37s\tremaining: 22.3s\n",
      "814:\tlearn: 0.0635512\ttotal: 1m 37s\tremaining: 22.2s\n",
      "815:\tlearn: 0.0635213\ttotal: 1m 38s\tremaining: 22.1s\n",
      "816:\tlearn: 0.0634859\ttotal: 1m 38s\tremaining: 22s\n",
      "817:\tlearn: 0.0634802\ttotal: 1m 38s\tremaining: 21.9s\n",
      "818:\tlearn: 0.0634449\ttotal: 1m 38s\tremaining: 21.7s\n",
      "819:\tlearn: 0.0634298\ttotal: 1m 38s\tremaining: 21.6s\n",
      "820:\tlearn: 0.0633942\ttotal: 1m 38s\tremaining: 21.5s\n",
      "821:\tlearn: 0.0633719\ttotal: 1m 38s\tremaining: 21.4s\n",
      "822:\tlearn: 0.0633448\ttotal: 1m 38s\tremaining: 21.2s\n",
      "823:\tlearn: 0.0633259\ttotal: 1m 38s\tremaining: 21.1s\n",
      "824:\tlearn: 0.0632961\ttotal: 1m 39s\tremaining: 21s\n",
      "825:\tlearn: 0.0632685\ttotal: 1m 39s\tremaining: 20.9s\n",
      "826:\tlearn: 0.0632546\ttotal: 1m 39s\tremaining: 20.8s\n",
      "827:\tlearn: 0.0632353\ttotal: 1m 39s\tremaining: 20.6s\n",
      "828:\tlearn: 0.0632214\ttotal: 1m 39s\tremaining: 20.5s\n",
      "829:\tlearn: 0.0631874\ttotal: 1m 39s\tremaining: 20.4s\n",
      "830:\tlearn: 0.0631584\ttotal: 1m 39s\tremaining: 20.3s\n",
      "831:\tlearn: 0.0631364\ttotal: 1m 39s\tremaining: 20.2s\n",
      "832:\tlearn: 0.0631192\ttotal: 1m 39s\tremaining: 20s\n",
      "833:\tlearn: 0.0630900\ttotal: 1m 40s\tremaining: 19.9s\n",
      "834:\tlearn: 0.0630472\ttotal: 1m 40s\tremaining: 19.8s\n",
      "835:\tlearn: 0.0630231\ttotal: 1m 40s\tremaining: 19.7s\n",
      "836:\tlearn: 0.0629703\ttotal: 1m 40s\tremaining: 19.6s\n",
      "837:\tlearn: 0.0629437\ttotal: 1m 40s\tremaining: 19.4s\n",
      "838:\tlearn: 0.0629062\ttotal: 1m 40s\tremaining: 19.3s\n",
      "839:\tlearn: 0.0628823\ttotal: 1m 40s\tremaining: 19.2s\n",
      "840:\tlearn: 0.0628422\ttotal: 1m 40s\tremaining: 19.1s\n",
      "841:\tlearn: 0.0628241\ttotal: 1m 41s\tremaining: 19s\n",
      "842:\tlearn: 0.0627935\ttotal: 1m 41s\tremaining: 18.8s\n",
      "843:\tlearn: 0.0627752\ttotal: 1m 41s\tremaining: 18.7s\n",
      "844:\tlearn: 0.0627578\ttotal: 1m 41s\tremaining: 18.6s\n",
      "845:\tlearn: 0.0627336\ttotal: 1m 41s\tremaining: 18.5s\n",
      "846:\tlearn: 0.0627134\ttotal: 1m 41s\tremaining: 18.4s\n",
      "847:\tlearn: 0.0626864\ttotal: 1m 41s\tremaining: 18.3s\n",
      "848:\tlearn: 0.0626728\ttotal: 1m 41s\tremaining: 18.1s\n",
      "849:\tlearn: 0.0626557\ttotal: 1m 42s\tremaining: 18s\n",
      "850:\tlearn: 0.0626405\ttotal: 1m 42s\tremaining: 17.9s\n",
      "851:\tlearn: 0.0626249\ttotal: 1m 42s\tremaining: 17.8s\n",
      "852:\tlearn: 0.0626141\ttotal: 1m 42s\tremaining: 17.7s\n",
      "853:\tlearn: 0.0625730\ttotal: 1m 42s\tremaining: 17.5s\n",
      "854:\tlearn: 0.0625326\ttotal: 1m 42s\tremaining: 17.4s\n",
      "855:\tlearn: 0.0624993\ttotal: 1m 42s\tremaining: 17.3s\n",
      "856:\tlearn: 0.0624873\ttotal: 1m 42s\tremaining: 17.2s\n",
      "857:\tlearn: 0.0624830\ttotal: 1m 43s\tremaining: 17.1s\n",
      "858:\tlearn: 0.0624611\ttotal: 1m 43s\tremaining: 16.9s\n",
      "859:\tlearn: 0.0624169\ttotal: 1m 43s\tremaining: 16.8s\n",
      "860:\tlearn: 0.0623943\ttotal: 1m 43s\tremaining: 16.7s\n",
      "861:\tlearn: 0.0623470\ttotal: 1m 43s\tremaining: 16.6s\n",
      "862:\tlearn: 0.0623229\ttotal: 1m 43s\tremaining: 16.5s\n",
      "863:\tlearn: 0.0622938\ttotal: 1m 43s\tremaining: 16.3s\n",
      "864:\tlearn: 0.0622807\ttotal: 1m 43s\tremaining: 16.2s\n",
      "865:\tlearn: 0.0622612\ttotal: 1m 44s\tremaining: 16.1s\n",
      "866:\tlearn: 0.0622524\ttotal: 1m 44s\tremaining: 16s\n",
      "867:\tlearn: 0.0622412\ttotal: 1m 44s\tremaining: 15.9s\n",
      "868:\tlearn: 0.0622186\ttotal: 1m 44s\tremaining: 15.7s\n",
      "869:\tlearn: 0.0622022\ttotal: 1m 44s\tremaining: 15.6s\n",
      "870:\tlearn: 0.0621791\ttotal: 1m 44s\tremaining: 15.5s\n",
      "871:\tlearn: 0.0621605\ttotal: 1m 44s\tremaining: 15.4s\n",
      "872:\tlearn: 0.0621418\ttotal: 1m 44s\tremaining: 15.3s\n",
      "873:\tlearn: 0.0621284\ttotal: 1m 44s\tremaining: 15.1s\n",
      "874:\tlearn: 0.0621191\ttotal: 1m 45s\tremaining: 15s\n",
      "875:\tlearn: 0.0621118\ttotal: 1m 45s\tremaining: 14.9s\n",
      "876:\tlearn: 0.0620803\ttotal: 1m 45s\tremaining: 14.8s\n",
      "877:\tlearn: 0.0620629\ttotal: 1m 45s\tremaining: 14.7s\n",
      "878:\tlearn: 0.0620410\ttotal: 1m 45s\tremaining: 14.5s\n",
      "879:\tlearn: 0.0620112\ttotal: 1m 45s\tremaining: 14.4s\n",
      "880:\tlearn: 0.0619853\ttotal: 1m 45s\tremaining: 14.3s\n",
      "881:\tlearn: 0.0619572\ttotal: 1m 45s\tremaining: 14.2s\n",
      "882:\tlearn: 0.0619381\ttotal: 1m 46s\tremaining: 14.1s\n",
      "883:\tlearn: 0.0619317\ttotal: 1m 46s\tremaining: 13.9s\n",
      "884:\tlearn: 0.0619156\ttotal: 1m 46s\tremaining: 13.8s\n",
      "885:\tlearn: 0.0618880\ttotal: 1m 46s\tremaining: 13.7s\n",
      "886:\tlearn: 0.0618739\ttotal: 1m 46s\tremaining: 13.6s\n",
      "887:\tlearn: 0.0618642\ttotal: 1m 46s\tremaining: 13.5s\n",
      "888:\tlearn: 0.0618543\ttotal: 1m 46s\tremaining: 13.3s\n",
      "889:\tlearn: 0.0618543\ttotal: 1m 46s\tremaining: 13.2s\n",
      "890:\tlearn: 0.0618276\ttotal: 1m 47s\tremaining: 13.1s\n",
      "891:\tlearn: 0.0617844\ttotal: 1m 47s\tremaining: 13s\n",
      "892:\tlearn: 0.0617776\ttotal: 1m 47s\tremaining: 12.9s\n",
      "893:\tlearn: 0.0617715\ttotal: 1m 47s\tremaining: 12.7s\n",
      "894:\tlearn: 0.0617522\ttotal: 1m 47s\tremaining: 12.6s\n",
      "895:\tlearn: 0.0617194\ttotal: 1m 47s\tremaining: 12.5s\n",
      "896:\tlearn: 0.0617051\ttotal: 1m 47s\tremaining: 12.4s\n",
      "897:\tlearn: 0.0616979\ttotal: 1m 47s\tremaining: 12.3s\n",
      "898:\tlearn: 0.0616678\ttotal: 1m 48s\tremaining: 12.1s\n",
      "899:\tlearn: 0.0616552\ttotal: 1m 48s\tremaining: 12s\n",
      "900:\tlearn: 0.0616392\ttotal: 1m 48s\tremaining: 11.9s\n",
      "901:\tlearn: 0.0616220\ttotal: 1m 48s\tremaining: 11.8s\n",
      "902:\tlearn: 0.0615955\ttotal: 1m 48s\tremaining: 11.7s\n",
      "903:\tlearn: 0.0615783\ttotal: 1m 48s\tremaining: 11.5s\n",
      "904:\tlearn: 0.0615763\ttotal: 1m 48s\tremaining: 11.4s\n",
      "905:\tlearn: 0.0615702\ttotal: 1m 48s\tremaining: 11.3s\n",
      "906:\tlearn: 0.0615402\ttotal: 1m 48s\tremaining: 11.2s\n",
      "907:\tlearn: 0.0615103\ttotal: 1m 49s\tremaining: 11s\n",
      "908:\tlearn: 0.0615102\ttotal: 1m 49s\tremaining: 10.9s\n",
      "909:\tlearn: 0.0614975\ttotal: 1m 49s\tremaining: 10.8s\n",
      "910:\tlearn: 0.0614846\ttotal: 1m 49s\tremaining: 10.7s\n",
      "911:\tlearn: 0.0614575\ttotal: 1m 49s\tremaining: 10.6s\n",
      "912:\tlearn: 0.0614137\ttotal: 1m 49s\tremaining: 10.4s\n",
      "913:\tlearn: 0.0614076\ttotal: 1m 49s\tremaining: 10.3s\n",
      "914:\tlearn: 0.0614062\ttotal: 1m 49s\tremaining: 10.2s\n",
      "915:\tlearn: 0.0613872\ttotal: 1m 49s\tremaining: 10.1s\n",
      "916:\tlearn: 0.0613555\ttotal: 1m 50s\tremaining: 9.96s\n",
      "917:\tlearn: 0.0613232\ttotal: 1m 50s\tremaining: 9.84s\n",
      "918:\tlearn: 0.0613114\ttotal: 1m 50s\tremaining: 9.72s\n",
      "919:\tlearn: 0.0612841\ttotal: 1m 50s\tremaining: 9.6s\n",
      "920:\tlearn: 0.0612520\ttotal: 1m 50s\tremaining: 9.48s\n",
      "921:\tlearn: 0.0612262\ttotal: 1m 50s\tremaining: 9.36s\n",
      "922:\tlearn: 0.0612022\ttotal: 1m 50s\tremaining: 9.24s\n",
      "923:\tlearn: 0.0611883\ttotal: 1m 50s\tremaining: 9.12s\n",
      "924:\tlearn: 0.0611704\ttotal: 1m 51s\tremaining: 9s\n",
      "925:\tlearn: 0.0611529\ttotal: 1m 51s\tremaining: 8.88s\n",
      "926:\tlearn: 0.0611422\ttotal: 1m 51s\tremaining: 8.76s\n",
      "927:\tlearn: 0.0611271\ttotal: 1m 51s\tremaining: 8.64s\n",
      "928:\tlearn: 0.0611181\ttotal: 1m 51s\tremaining: 8.52s\n",
      "929:\tlearn: 0.0610948\ttotal: 1m 51s\tremaining: 8.4s\n",
      "930:\tlearn: 0.0610657\ttotal: 1m 51s\tremaining: 8.28s\n",
      "931:\tlearn: 0.0610557\ttotal: 1m 51s\tremaining: 8.16s\n",
      "932:\tlearn: 0.0610359\ttotal: 1m 51s\tremaining: 8.04s\n",
      "933:\tlearn: 0.0610173\ttotal: 1m 52s\tremaining: 7.92s\n",
      "934:\tlearn: 0.0610050\ttotal: 1m 52s\tremaining: 7.8s\n",
      "935:\tlearn: 0.0609785\ttotal: 1m 52s\tremaining: 7.68s\n",
      "936:\tlearn: 0.0609412\ttotal: 1m 52s\tremaining: 7.56s\n",
      "937:\tlearn: 0.0609175\ttotal: 1m 52s\tremaining: 7.44s\n",
      "938:\tlearn: 0.0608935\ttotal: 1m 52s\tremaining: 7.32s\n",
      "939:\tlearn: 0.0608495\ttotal: 1m 52s\tremaining: 7.2s\n",
      "940:\tlearn: 0.0608323\ttotal: 1m 52s\tremaining: 7.08s\n",
      "941:\tlearn: 0.0608214\ttotal: 1m 53s\tremaining: 6.96s\n",
      "942:\tlearn: 0.0608067\ttotal: 1m 53s\tremaining: 6.84s\n",
      "943:\tlearn: 0.0607884\ttotal: 1m 53s\tremaining: 6.72s\n",
      "944:\tlearn: 0.0607703\ttotal: 1m 53s\tremaining: 6.6s\n",
      "945:\tlearn: 0.0607428\ttotal: 1m 53s\tremaining: 6.48s\n",
      "946:\tlearn: 0.0607264\ttotal: 1m 53s\tremaining: 6.36s\n",
      "947:\tlearn: 0.0607214\ttotal: 1m 53s\tremaining: 6.24s\n",
      "948:\tlearn: 0.0607042\ttotal: 1m 53s\tremaining: 6.12s\n",
      "949:\tlearn: 0.0606920\ttotal: 1m 53s\tremaining: 6s\n",
      "950:\tlearn: 0.0606776\ttotal: 1m 54s\tremaining: 5.88s\n",
      "951:\tlearn: 0.0606590\ttotal: 1m 54s\tremaining: 5.75s\n",
      "952:\tlearn: 0.0606504\ttotal: 1m 54s\tremaining: 5.63s\n",
      "953:\tlearn: 0.0606361\ttotal: 1m 54s\tremaining: 5.51s\n",
      "954:\tlearn: 0.0606086\ttotal: 1m 54s\tremaining: 5.39s\n",
      "955:\tlearn: 0.0605890\ttotal: 1m 54s\tremaining: 5.27s\n",
      "956:\tlearn: 0.0605698\ttotal: 1m 54s\tremaining: 5.15s\n",
      "957:\tlearn: 0.0605504\ttotal: 1m 54s\tremaining: 5.03s\n",
      "958:\tlearn: 0.0605232\ttotal: 1m 54s\tremaining: 4.91s\n",
      "959:\tlearn: 0.0605145\ttotal: 1m 55s\tremaining: 4.79s\n",
      "960:\tlearn: 0.0604949\ttotal: 1m 55s\tremaining: 4.67s\n",
      "961:\tlearn: 0.0604871\ttotal: 1m 55s\tremaining: 4.55s\n",
      "962:\tlearn: 0.0604775\ttotal: 1m 55s\tremaining: 4.43s\n",
      "963:\tlearn: 0.0604657\ttotal: 1m 55s\tremaining: 4.32s\n",
      "964:\tlearn: 0.0604648\ttotal: 1m 55s\tremaining: 4.19s\n",
      "965:\tlearn: 0.0604557\ttotal: 1m 55s\tremaining: 4.07s\n",
      "966:\tlearn: 0.0604198\ttotal: 1m 55s\tremaining: 3.95s\n",
      "967:\tlearn: 0.0604068\ttotal: 1m 56s\tremaining: 3.83s\n",
      "968:\tlearn: 0.0603877\ttotal: 1m 56s\tremaining: 3.71s\n",
      "969:\tlearn: 0.0603728\ttotal: 1m 56s\tremaining: 3.6s\n",
      "970:\tlearn: 0.0603703\ttotal: 1m 56s\tremaining: 3.48s\n",
      "971:\tlearn: 0.0603460\ttotal: 1m 56s\tremaining: 3.35s\n",
      "972:\tlearn: 0.0603311\ttotal: 1m 56s\tremaining: 3.24s\n",
      "973:\tlearn: 0.0602994\ttotal: 1m 56s\tremaining: 3.12s\n",
      "974:\tlearn: 0.0602781\ttotal: 1m 56s\tremaining: 3s\n",
      "975:\tlearn: 0.0602665\ttotal: 1m 56s\tremaining: 2.88s\n",
      "976:\tlearn: 0.0602515\ttotal: 1m 57s\tremaining: 2.76s\n",
      "977:\tlearn: 0.0602350\ttotal: 1m 57s\tremaining: 2.64s\n",
      "978:\tlearn: 0.0602008\ttotal: 1m 57s\tremaining: 2.52s\n",
      "979:\tlearn: 0.0601824\ttotal: 1m 57s\tremaining: 2.4s\n",
      "980:\tlearn: 0.0601663\ttotal: 1m 57s\tremaining: 2.28s\n",
      "981:\tlearn: 0.0601506\ttotal: 1m 57s\tremaining: 2.16s\n",
      "982:\tlearn: 0.0601440\ttotal: 1m 57s\tremaining: 2.04s\n",
      "983:\tlearn: 0.0601347\ttotal: 1m 57s\tremaining: 1.92s\n",
      "984:\tlearn: 0.0601234\ttotal: 1m 58s\tremaining: 1.8s\n",
      "985:\tlearn: 0.0600667\ttotal: 1m 58s\tremaining: 1.68s\n",
      "986:\tlearn: 0.0600585\ttotal: 1m 58s\tremaining: 1.56s\n",
      "987:\tlearn: 0.0600336\ttotal: 1m 58s\tremaining: 1.44s\n",
      "988:\tlearn: 0.0600208\ttotal: 1m 58s\tremaining: 1.32s\n",
      "989:\tlearn: 0.0600111\ttotal: 1m 58s\tremaining: 1.2s\n",
      "990:\tlearn: 0.0599593\ttotal: 1m 58s\tremaining: 1.08s\n",
      "991:\tlearn: 0.0599461\ttotal: 1m 59s\tremaining: 960ms\n",
      "992:\tlearn: 0.0599319\ttotal: 1m 59s\tremaining: 840ms\n",
      "993:\tlearn: 0.0599261\ttotal: 1m 59s\tremaining: 720ms\n",
      "994:\tlearn: 0.0599113\ttotal: 1m 59s\tremaining: 600ms\n",
      "995:\tlearn: 0.0599046\ttotal: 1m 59s\tremaining: 480ms\n",
      "996:\tlearn: 0.0598813\ttotal: 1m 59s\tremaining: 360ms\n",
      "997:\tlearn: 0.0598750\ttotal: 1m 59s\tremaining: 240ms\n",
      "998:\tlearn: 0.0598501\ttotal: 1m 59s\tremaining: 120ms\n",
      "999:\tlearn: 0.0598379\ttotal: 1m 59s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "submission = pd.concat([submission, pd.DataFrame(y_pred[:, 1], columns=['isFraud'])], axis=1)\n",
    "submission.to_csv(root + \"/submissions/catboost_ieee_fraud_detection.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}